{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Aggression Neural Network (DANN)\n",
    "\n",
    "Driver Aggression Neural Network is assigning an aggression value to a sorted set of sensory data. Driving is simulated in BeamNG v0.27 using their BeamNGpy open-source library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parquet_file_path = 'imu_data_2023_04_30_23_41_59.parquet'\n",
    "data = pd.read_parquet(parquet_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify training data\n",
    "\n",
    "- Group together data recorded from the same sensor\n",
    "- Take around 100-1000 recorded data without the aggression values\n",
    "- Make aggression values the label of the dataset\n",
    "- Create a lot of training data by chunking the sorted (by timestamp) records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time      mass  accSmoothX  accSmoothY  accSmoothZ  angAccelX  angAccelY  angAccelZ\n",
      "0    0.00000  28.76814  -12.850381   -9.608240   -8.279416 -31.915836  -0.945196  11.738538\n",
      "1    0.01025  28.76814    1.919665    1.875409  -11.425454  11.985555   5.752812  -6.672496\n",
      "2    0.02050  28.76814   -3.467129    6.821073  -43.264269 -17.679147  -8.892725 -18.515280\n",
      "3    0.03075  28.76814   -1.516496   -2.040845  -30.195730 -15.668750  -7.214118  -6.147648\n",
      "4    0.07175  28.76814    3.982196   -1.018411   24.312185  14.425741   4.802859   5.597455\n",
      "5    0.08200  28.76814   -1.225139   18.315980   12.266464   5.600663  -0.856092   3.836470\n",
      "6    0.09225  28.76814   -4.443646   19.020500   -4.328997   0.497755  -0.350534  -5.024224\n",
      "7    0.13325  28.76814   -5.167698   11.847199    1.470731   2.203150   6.553607  -1.107597\n",
      "8    0.14350  28.76814    1.025816   -4.852378   -3.214385  -1.465744  -1.560213  -2.210415\n",
      "9    0.15375  28.76814   -0.581155    3.951569  -16.458101 -12.479302   0.852011  -5.345779\n",
      "10   0.16400  28.76814    6.367672    4.774746   -6.024596  -3.012921 -11.148322   3.166777\n",
      "11   0.20500  28.76814    8.459531   -7.849384  -13.333083   3.901367  -7.889077  -9.850002\n",
      "12   0.21525  28.76814    1.256899    8.245398  -14.518803  -6.161231   0.529895  -3.322777\n",
      "13   0.22550  28.76814   -5.036533   -5.581864    1.652816 -13.911338   0.922776  10.466599\n",
      "14   0.26650  28.76814   -1.700033    0.529080  -21.277647  -3.767055   2.391041 -14.000220\n",
      "15   0.27675  28.76814    4.219901   -4.798086    4.095576  12.682738   5.589621   0.466422\n",
      "16   0.28700  28.76814    2.273287    7.697083   18.729442   6.000308   3.376489   5.971565\n",
      "17   0.29725  28.76814   -1.456261    7.157701    2.577912 -11.171239  -3.902681   0.762942\n",
      "18   0.33825  28.76814   10.072677    6.480465  -10.687030  13.692153   3.283079 -12.583006\n",
      "19   0.34850  28.76814    6.449548    4.155573   10.357222   4.230245  -2.255573   3.789610\n",
      "20   0.35875  28.76814    0.843569    6.505029   14.573258   4.246709   4.244371   3.121197\n",
      "21   0.39975  28.76814   -0.493783   -8.016083   -7.879784  -3.608638  -3.204911   2.375314\n",
      "22   0.41000  28.76814    9.656839  -11.526378   39.258896   9.590738   8.573834  17.438601\n",
      "23   0.42025  28.76814   11.805527  -10.701652   14.243395  22.602135  15.635391 -10.675639\n",
      "24   0.43050  28.76814    0.201760    2.461635  -40.375617   4.168812   2.864092 -30.142816\n",
      "25   0.47150  28.76814    6.126532   -4.329840   32.945548   5.305227   4.735752  13.012681\n",
      "26   0.48175  28.76814    4.189742    1.379002   -6.530214   1.991945   5.048357 -12.507855\n",
      "27   0.49200  28.76814   -3.726521    4.421608  -28.929109  -2.261265   2.048290 -16.463057\n",
      "28   0.53300  28.76814   -0.246799    4.839789   15.726459  -4.986486  -0.488089   9.247012\n",
      "29   0.54325  28.76814   -0.958140    3.558962    3.721336  -1.039031  -2.929533   3.589476\n",
      "30   0.55350  28.76814    3.351961    1.765082  -11.182807   9.031402   3.802022  -8.280944\n",
      "31   0.56375  28.76814   11.473506   -3.846030   -8.507995   5.626781  -2.220128  -6.022686\n",
      "32   0.60475  28.76814    3.176873   -4.852318   -1.597714 -11.403606  -3.498901   1.099505\n",
      "33   0.61500  28.76814    5.968210   -1.179261  -14.686193  -5.240121  -3.873769  -5.375677\n",
      "34   0.62525  28.76814   -0.888006    0.490100  -17.847866  -3.591874  -3.956181  -4.154010\n",
      "35   0.66625  28.76814   -2.052258   -4.232520   -5.071476 -14.258044  -4.326270   3.957733\n",
      "36   0.67650  28.76814   -2.055690    1.403401   -8.236095  -9.932037  -5.113896   2.531605\n",
      "37   0.68675  28.76814    5.105728   -3.761421   11.879644   1.169292   2.366436  10.419995\n",
      "38   0.69700  28.76814    6.301086    5.490359    7.304086  14.369789   1.067857  -1.771032\n",
      "39   0.73800  28.76814   -2.705576    2.158959    8.027458  -3.513145  -2.658645   4.918491\n",
      "40   0.74825  28.76814   -1.435517    5.919789    1.946272   0.027195   1.393651  -0.653969\n",
      "41   0.75850  28.76814    3.074554    0.767258   -1.876522   3.702126  -0.326168  -3.428259\n",
      "42   0.79950  28.76814    3.675574   -3.996334    5.299314   3.617312  -1.396924   2.060579\n",
      "43   0.80975  28.76814   -1.845552    0.546692   -1.868649  -0.242998   1.433784  -4.033663\n",
      "44   0.82000  28.76814   -1.987883   -0.405341    4.242613  -0.867805   1.475409   3.373737\n",
      "45   0.83025  28.76814   -0.350752    0.742097    5.055568  -1.584253  -0.368252   4.569253\n",
      "46   0.87125  28.76814    0.896391    2.010032    3.764863   3.165881   2.442251  -0.817923\n",
      "47   0.88150  28.76814   -1.949709    1.074456    0.879027  -3.652330  -1.169872   2.599445\n",
      "48   0.89175  28.76814   -1.314811    0.597329   -3.363205  -2.244157  -0.705935  -1.029232\n",
      "49   0.93275  28.76814   -0.281010   -2.498728   -3.683009  -3.281011  -0.621543   0.061921\n",
      "50   0.94300  28.76814    0.708187   -0.474497   -0.824059  -1.514535  -1.088531  -0.455790\n",
      "51   0.95325  28.76814    0.484834    0.551053    1.396402   3.418309   0.950660  -1.313382\n",
      "52   0.96350  28.76814   -0.067166    1.316969    0.096184  -0.195611  -0.026473   0.343419\n",
      "53   1.00450  28.76814    0.569406   -0.197632    1.922841  -2.042790  -0.906351   2.611403\n",
      "54   1.01475  28.76814    1.950135   -0.843271    2.535635   1.607223   0.003941   0.543510\n",
      "55   1.02500  28.76814   -0.304206   -2.510852    0.424996   1.942804   0.961206  -0.812966\n",
      "56   1.06600  28.76814   -0.979682    1.042693   -0.803420  -2.331438  -0.995903   0.882068\n",
      "57   1.07625  28.76814    1.343401    0.401590    2.502196   1.517585   0.776785   1.611056\n",
      "58   1.08650  28.76814    1.378099    1.571198    0.889960  -0.558921  -1.036467   0.699727\n",
      "59   1.09675  28.76814   -0.984417    0.212585   -0.139774   0.002156  -0.048031  -1.476631\n",
      "60   1.13775  28.76814    0.141271   -1.572559    0.640531   2.767456   1.099977  -1.280122\n",
      "61   1.14800  28.76814    2.803153   -3.027157    0.733977   1.457759   0.047724  -0.410205\n",
      "62   1.15825  28.76814    0.471323   -0.174592    1.646573   0.317718  -0.259042   1.715609\n",
      "63   1.19925  28.76814   -0.210526    1.161435    1.227215   1.764112   0.297877  -0.519733\n",
      "64   1.20950  28.76814   -0.358600    0.434399    1.263369  -2.711133  -1.205801   2.487347\n",
      "65   1.21975  28.76814    1.011136   -0.056215    1.952930  -0.016433  -0.085838   0.703372\n",
      "66   1.23000  28.76814    0.681517    0.681021   -1.033372   0.667351  -0.018490  -1.279793\n",
      "67   1.27100  28.76814   -1.454309   -0.494323    0.739712  -0.993700  -0.844583   1.616180\n",
      "68   1.28125  28.76814   -0.720785   -0.627397    0.063141   0.136613   0.528560  -0.399304\n",
      "69   1.29150  28.76814   -0.439423    0.495134   -1.496950  -1.700641  -0.390611   0.358756\n",
      "70   1.33250  28.76814    0.918638   -0.253307   -0.653382  -1.265670  -0.951782  -0.368670\n",
      "71   1.34275  28.76814   -2.612262    1.136634   -1.165235   0.693613   0.545485  -2.115305\n",
      "72   1.35300  28.76814   -1.757486    0.380910   -0.398053  -0.515501   0.160306   1.082334\n",
      "73   1.36325  28.76814    1.357426    0.351971    0.367680  -0.495736  -1.005717   1.645952\n",
      "74   1.40425  28.76814   -0.574661    0.597513   -1.639158   0.236766   0.948600  -2.231651\n",
      "75   1.41450  28.76814   -2.097941    0.414333   -3.277269  -3.239223  -0.975422  -0.683809\n",
      "76   1.42475  28.76814   -0.877578    0.934679    0.299457  -0.062285   0.550480  -0.190249\n",
      "77   1.46575  28.76814    1.308113   -0.068867   -0.864207   0.733020   0.818516  -1.286209\n",
      "78   1.47600  28.76814    0.563590   -0.161495   -2.361382  -1.064303  -0.894287   0.236876\n",
      "79   1.48625  28.76814   -0.698692   -0.310505   -0.770840   0.430436  -0.477458  -0.346395\n",
      "80   1.49650  28.76814    0.693830   -1.384668    0.476761   2.052531   0.715267   0.271286\n",
      "81   1.53750  28.76814    0.502218    0.398285   -2.429226  -2.884789  -0.718829  -0.243981\n",
      "82   1.54775  28.76814    0.921564    0.323342   -0.133361   1.690675  -0.013773  -1.198716\n",
      "83   1.55800  28.76814   -0.505819    1.032494    0.056610   1.472268   0.220184  -0.887104\n",
      "84   1.59900  28.76814   -1.086678    0.227956   -2.243023  -2.607229  -1.680212   1.235294\n",
      "85   1.60925  28.76814    1.452749   -1.589732   -0.236291   1.551452   0.879670   0.560551\n",
      "86   1.61950  28.76814    0.427234   -1.621420   -1.422789   1.012834   0.159998   0.024365\n",
      "87   1.62975  28.76814   -0.765031   -0.189003    0.400206  -0.188047  -0.244354   0.156879\n",
      "88   1.67075  28.76814   -1.108353    1.532401    2.638508   2.643758   1.275559  -0.117187\n",
      "89   1.68100  28.76814    1.054251    0.511005    1.064740  -0.361895   0.101112   0.305019\n",
      "90   1.69125  28.76814    0.305872    0.202542    0.083999  -1.047702  -1.014056   0.414709\n",
      "91   1.73225  28.76814    0.492933   -0.942190   -0.264134   2.319408   0.748606  -1.136430\n",
      "92   1.74250  28.76814   -0.823291   -0.823360   -0.335068  -1.572238  -0.353208   0.879190\n",
      "93   1.75275  28.76814    1.811865   -0.923228    0.613908   0.725315   1.000638  -0.536629\n",
      "94   1.76300  28.76814   -0.713383   -0.199236   -1.662679  -0.378550   0.494068  -1.316335\n",
      "95   1.80400  28.76814   -1.533662   -0.207073    1.770011  -1.401721  -0.517203   2.086123\n",
      "96   1.81425  28.76814   -1.371657   -0.082507    1.349162   0.238127   0.157268   1.543975\n",
      "97   1.82450  28.76814    0.778130   -0.568090   -1.115190   0.135858   0.277779  -0.691155\n",
      "98   1.87575  28.76814   -0.676034   -0.233329    1.556883   0.910884   1.061801   0.520609\n",
      "99   1.88600  28.76814   -1.707610    0.213020   -2.404550  -2.471342  -0.603215   0.215861\n",
      "100  1.89625  28.76814    0.761326    0.639697   -0.610850  -1.195616  -0.159408  -0.397905\n",
      "101  1.93725  28.76814    0.117952    0.286851    0.990274   1.088564   0.202423  -0.745681\n",
      "102  1.94750  28.76814   -1.567557   -0.507125   -1.827186  -1.229316  -0.421068  -0.214946\n",
      "103  1.95775  28.76814   -0.156132   -0.931164   -1.109258  -0.963279  -0.410742   0.409095\n",
      "104  2.00900  28.76814   -0.540826    1.420132   -1.112412  -2.240128  -1.133500  -0.833259\n",
      "105  2.01925  28.76814   -1.583690    1.623183    0.504230   0.769083   0.902839  -1.536052\n",
      "106  2.02950  28.76814    0.223360    0.563277    1.355618   1.024786   0.491730   0.209282\n",
      "107  2.07050  28.76814    0.148715   -0.344474    0.918608  -1.138155  -0.534901   2.550213\n",
      "108  2.08075  28.76814   -0.664559   -0.514591   -0.915242   1.062772   0.517879  -1.597690\n",
      "109  2.09100  28.76814   -1.712416   -0.602588   -3.232067  -1.485876  -0.492346  -1.229233\n",
      "110  2.14225  28.76814    0.742863    1.265671    4.062566   2.763458   0.286383   0.133296\n",
      "111  2.15250  28.76814   -0.624537   -1.052055    4.722086  -1.539059  -0.948232   1.927800\n",
      "112  2.16275  28.76814   -0.797171    0.852890   -0.786570  -0.093747   0.098516   0.692216\n",
      "113  2.20375  28.76814   -0.821061   -1.302077    1.847925   3.048795   1.098966   0.232455\n",
      "114  2.21400  28.76814    1.645967    0.379861    2.671966   1.651545  -1.651789   0.661145\n",
      "115  2.22425  28.76814    0.212564   -0.708782   -1.317594  -3.024376  -0.213264   0.830389\n",
      "116  2.27550  28.76814    0.282327   -0.039800    3.838056   3.641935   1.043227   0.515134\n",
      "117  2.28575  28.76814    1.308646   -0.434316    1.061559  -0.115277   0.491221   2.395209\n",
      "118  2.29600  28.76814    3.016153    0.294335    3.409643  -2.093313  -0.851760   3.010006\n",
      "119  2.33700  28.76814    5.535644   -6.598213   -5.695990  -0.411244  -0.012001  -5.076289\n",
      "120  2.34725  28.76814    5.333715   -0.227603    1.893725   4.540085   0.752411  -1.866811\n",
      "121  2.35750  28.76814    4.952651   -1.440804   11.321128   0.759080  -1.346824   6.450318\n",
      "122  2.40875  28.76814    3.307316    3.009121    4.193579   2.827171  -2.173802   0.943172\n",
      "123  2.41900  28.76814    2.737474    2.090440   -9.386289  -0.164167   1.271797  -5.154857\n",
      "124  2.42925  28.76814    1.435468   -0.870524   -8.273693  -1.923130  -1.438372  -2.644555\n",
      "125  2.47025  28.76814    3.923428    0.904194   -3.280686  -6.515306  -4.123546   0.895054\n",
      "126  2.48050  28.76814   -2.868457    3.107865   -5.907970  -4.358701  -0.171346  -2.605019\n",
      "127  2.49075  28.76814    3.063211    0.539220   -0.009323   2.852602   4.068258  -1.038997\n",
      "128  2.54200  28.76814    4.624777   -4.196317    6.567234  11.553319   5.578832  -3.447600\n",
      "129  2.55225  28.76814    3.990508   -0.635562   -7.216060  -0.798775  -0.702975  -5.900237\n",
      "130  2.56250  28.76814   -0.436222   -6.214876   -7.983478  -9.466858  -3.313595  -0.256482\n",
      "131  2.60350  28.76814   -0.420035    2.980393  -12.082593  -1.888295   1.319254 -11.302393\n",
      "132  2.61375  28.76814    1.648829   -1.238544   -7.813209  -8.315068  -5.521269   0.785877\n",
      "133  2.62400  28.76814    2.091515   -2.525903   13.717534   2.834226   1.790374  10.702089\n",
      "134  2.67525  28.76814    4.996611   -1.097989    5.127085   7.448980   3.603942   0.199139\n",
      "135  2.68550  28.76814    0.707541    5.522374   -5.223588   1.839782   0.617107  -3.952449\n",
      "136  2.69575  28.76814    3.056990   -0.133326   -5.520544  -5.534105  -3.339754  -2.560076\n",
      "137  2.73675  28.76814    0.207700   -2.460817   -5.606759  -9.929727  -3.792742   0.819690\n",
      "138  2.74700  28.76814    1.145196    1.024465  -10.122820  -2.335575  -2.203077  -7.173723\n",
      "139  2.75725  28.76814    5.193288    2.276158    8.666384   9.882038   5.125917   2.217198\n",
      "140  2.80850  28.76814    2.413903   -0.438111    6.915951   4.276513   0.887575   1.925930\n",
      "141  2.81875  28.76814    0.479943    0.425179   -4.481102 -10.077720  -4.545324   1.574424\n",
      "142  2.82900  28.76814    1.634261   -0.050049   -6.066415  -1.422515   0.266415  -3.317468\n",
      "143  2.87000  28.76814    3.335183   -0.369184    0.316584   3.225792   0.952789  -2.495431\n",
      "144  2.88025  28.76814    2.732125   -2.554198   -5.172453   1.986604   0.650414  -3.624008\n",
      "145  2.89050  28.76814    4.316673   -2.474130   -3.552890  -3.983944  -4.506471   1.207094\n",
      "146  2.94175  28.76814    3.971148    2.874859    4.576936   4.517314   2.160001  -2.395028\n",
      "147  2.95200  28.76814    3.021120   -0.558674   -7.478174   1.947583   0.289922  -6.310941\n",
      "148  2.96225  28.76814    0.408421   -3.380199   -7.087759  -8.214634  -4.786799   2.160659\n",
      "149  3.00325  28.76814   -1.120191   -1.929405    1.744094  -4.493243   0.386374   4.252443\n",
      "150  3.01350  28.76814    0.068248    1.765912    6.566965   0.994327   0.919764   3.448307\n",
      "151  3.02375  28.76814    0.700223    0.763652    7.863707   3.408414   4.959314   0.920435\n",
      "152  3.07500  28.76814    5.204544    2.063285    0.738371  -0.092941  -1.787927   2.150659\n",
      "153  3.08525  28.76814    3.009830   -4.070823   -5.539505   2.997128   2.202292  -6.936695\n",
      "154  3.09550  28.76814    2.058514   -0.515273    0.566307  -5.312050  -5.166709   3.674942\n",
      "155  3.13650  28.76814    2.930676   -0.760174   -4.845655  -5.885180  -1.303058   1.340462\n",
      "156  3.14675  28.76814    2.110414   -4.599911  -11.850434  -2.143735  -2.855093  -8.634873\n",
      "157  3.15700  28.76814    1.882470    0.902301    0.233872  -3.586127  -1.846557   3.448737\n",
      "158  3.20825  28.76814    1.405787   -3.214908  -11.763149  -0.335623  -1.192446  -5.340988\n",
      "159  3.21850  28.76814    2.534626    2.324895   -0.040867  -1.717867  -3.165213   3.470955\n",
      "160  3.22875  28.76814    4.760022    0.685805   15.441097  -0.504206   0.572555   7.991894\n",
      "161  3.26975  28.76814    2.286953   -1.466743  -10.197215   1.803218   1.471911  -8.593500\n",
      "162  3.28000  28.76814    0.319942   -1.113548  -13.244426  -6.956587  -3.758887  -4.889698\n",
      "163  3.29025  28.76814    3.910839    6.120761   -1.074042   0.485249  -5.809894   2.864383\n",
      "164  3.34150  28.76814    0.824305   -1.541110   13.207415  -0.793185   4.053653   5.306553\n",
      "165  3.35175  28.76814   -1.861024    0.117998   -2.499512  -2.274465   0.498450  -2.978094\n",
      "166  3.36200  28.76814   -0.294022    3.024747  -10.390768  -3.798363  -3.022201  -1.633118\n",
      "167  3.40300  28.76814    0.107941    0.451313    2.705808  -0.639819  -0.563045  -0.083249\n",
      "168  3.41325  28.76814   -0.973770    2.802282    3.726999   0.065915   0.167577   1.076669\n",
      "169  3.42350  28.76814   -0.358769    0.948310    0.758005  -0.735664   1.256487   0.631791\n",
      "170  3.47475  28.76814   -0.257116   -0.807977    1.869682   0.216507   1.181764   1.744120\n",
      "171  3.48500  28.76814   -0.150174    0.577488    1.442638  -0.744316  -0.908480   2.972511\n",
      "172  3.49525  28.76814    0.116750   -0.029777   -0.620144  -0.943566  -0.977700   0.598011\n",
      "173  3.53625  28.76814   -1.560628   -1.726995    1.375393  -1.362994   1.310166   2.248157\n",
      "174  3.54650  28.76814    0.241072    0.043044    1.245174  -0.270139  -1.644046   1.758664\n",
      "175  3.55675  28.76814    0.258268   -1.168621   -0.827552  -0.120453  -0.598799  -0.723487\n",
      "176  3.60800  28.76814    0.663812   -0.689611    0.949210   0.461070  -1.231579   0.829409\n",
      "177  3.61825  28.76814    0.980497    0.079493   -1.511486   0.012170  -1.110241  -1.505701\n",
      "178  3.62850  28.76814   -0.152347    1.375510   -1.160835  -0.232874  -0.865438  -1.321825\n",
      "179  3.66950  28.76814    0.841968    0.698798   -0.189308   0.179371  -0.202816  -0.059342\n",
      "180  3.67975  28.76814    0.258501   -0.569855   -2.967451  -1.940400  -0.366690  -1.754716\n",
      "181  3.69000  28.76814   -0.426368    0.665879   -0.798867  -0.526853   0.039894  -0.136176\n",
      "182  3.74125  28.76814    0.502850   -0.925690   -2.179307  -0.514618  -0.077311  -2.540974\n",
      "183  3.75150  28.76814   -0.191523    1.119847    3.071070   1.626558   1.019233   1.209460\n",
      "184  3.76175  28.76814   -1.159997    1.122495    1.504192   0.747882   1.205441  -0.045602\n",
      "185  3.80275  28.76814    1.001005   -0.725529    1.625867   1.330601   0.002958  -0.938708\n",
      "186  3.81300  28.76814   -0.404784   -0.856828    3.707016   1.416991   2.096008   0.070594\n",
      "187  3.82325  28.76814    0.002395    1.658401    2.117445   0.426784   0.632399   0.457811\n",
      "188  3.87450  28.76814    1.217180   -1.149145    3.167961   1.691574   0.581438   0.603017\n",
      "189  3.88475  28.76814   -0.501730   -1.279138    0.888387  -1.744900   0.424251   1.337661\n",
      "190  3.89500  28.76814   -0.539636    0.133161   -1.262396  -1.273586  -1.499362   0.676333\n",
      "191  3.93600  28.76814   -1.146757   -0.489166    1.146717  -0.208096   1.861358   0.732716\n",
      "192  3.94625  28.76814   -1.308232    0.550024   -0.378688  -1.293558   0.119528   0.839201\n",
      "193  3.95650  28.76814    0.318254   -1.732300   -1.324648  -0.536482  -0.139745   0.885872\n",
      "194  4.00775  28.76814   -0.185780   -0.117660    0.589972  -1.267548  -0.249983   1.766426\n",
      "195  4.01800  28.76814    0.142522   -0.851449   -1.465577   0.472781  -0.509525  -1.547268\n",
      "196  4.02825  28.76814    1.430649    0.121930   -2.491790   0.379264  -0.512470  -3.292854\n",
      "197  4.06925  28.76814    1.285403   -2.482643    0.497054   0.863360  -0.442066  -0.790721\n",
      "198  4.07950  28.76814    2.489301   -1.183663   -1.119022   1.245628  -0.680822  -2.031363\n",
      "199  4.08975  28.76814    1.186341    0.256026   -0.340714  -0.366513  -0.617283  -2.002106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is your Scheme\n",
    "# data = pd.DataFrame(columns=[\n",
    "#     'imuId',\n",
    "#     'vehicleAggression',\n",
    "#     'time',\n",
    "#     'pos',\n",
    "#     'dirX',\n",
    "#     'dirY',\n",
    "#     'dirZ',\n",
    "#     'angVel',\n",
    "#     'angAccel',\n",
    "#     'mass',\n",
    "#     'accRaw',\n",
    "#     'accSmooth'\n",
    "# ])\n",
    "\n",
    "# Function to split the data into chunks\n",
    "def split_into_chunks(data, chunk_size):\n",
    "    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "# Group the data by 'imuId' and sort within each group by 'time'\n",
    "grouped_data = data.groupby('imuId').apply(lambda x: x.sort_values('time')).reset_index(drop=True)\n",
    "\n",
    "# Set the desired chunk size (number of records per chunk)\n",
    "chunk_size = 200\n",
    "\n",
    "# Split the data into chunks and assign the 'vehicleAggression' value as the label\n",
    "training_data = []\n",
    "for imu_id, group in grouped_data.groupby('imuId'):\n",
    "    chunks = split_into_chunks(group, chunk_size)\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) >= chunk_size:\n",
    "            label = chunk['vehicleAggression'].iloc[0]\n",
    "            first_timestamp = chunk['time'].iloc[0]\n",
    "            adjusted_time = chunk['time'] - first_timestamp\n",
    "            \n",
    "            # Separate list columns into individual columns\n",
    "            # pos_df = pd.DataFrame(chunk['pos'].tolist(), columns=['posX', 'posY', 'posZ'], index=chunk.index)\n",
    "            # dir_x_df = pd.DataFrame(chunk['dirX'].tolist(), columns=['dirXX', 'dirXY', 'dirXZ'], index=chunk.index)\n",
    "            # dir_y_df = pd.DataFrame(chunk['dirY'].tolist(), columns=['dirYX', 'dirYY', 'dirYZ'], index=chunk.index)\n",
    "            # dir_z_df = pd.DataFrame(chunk['dirZ'].tolist(), columns=['dirZX', 'dirZY', 'dirZZ'], index=chunk.index)\n",
    "            # acc_raw_df = pd.DataFrame(chunk['accRaw'].tolist(), columns=['accRawX', 'accRawY', 'accRawZ'], index=chunk.index)\n",
    "            acc_smooth_df = pd.DataFrame(chunk['accSmooth'].tolist(), columns=['accSmoothX', 'accSmoothY', 'accSmoothZ'], index=chunk.index)\n",
    "            # ang_vel_df = pd.DataFrame(chunk['angVel'].tolist(), columns=['angVelX', 'angVelY', 'angVelZ'], index=chunk.index)\n",
    "            ang_accel_df = pd.DataFrame(chunk['angAccel'].tolist(), columns=['angAccelX', 'angAccelY', 'angAccelZ'], index=chunk.index)\n",
    "            \n",
    "            expanded_chunk = pd.concat(\n",
    "                [\n",
    "                    chunk,\n",
    "                    # pos_df,\n",
    "                    # dir_x_df,\n",
    "                    # dir_y_df,\n",
    "                    # dir_z_df,\n",
    "                    # acc_raw_df,\n",
    "                    acc_smooth_df,\n",
    "                    # ang_vel_df,\n",
    "                    ang_accel_df\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            updated_chunk = (\n",
    "                expanded_chunk.assign(time=adjusted_time)\n",
    "                .drop(['imuId', 'vehicleAggression', 'pos', 'dirX', 'dirY', 'dirZ', 'angVel', 'angAccel', 'accRaw', 'accSmooth'], axis=1)\n",
    "            )\n",
    "            \n",
    "            training_data.append({'data': updated_chunk, 'label': label})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "training_data_df = pd.DataFrame(training_data)\n",
    "\n",
    "# Example of a single training set\n",
    "# Set display options\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 100)  # Set the maximum column width to 100 characters\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "print(training_data_df.loc[0, 'data'])\n",
    "pd.reset_option(\"all\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating the training data to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (449, 200, 8)\n",
      "X_test shape: (113, 200, 8)\n",
      "y_train shape: (449,)\n",
      "y_test shape: (113,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the data and labels from the training_data_df\n",
    "X = np.stack(training_data_df['data'].apply(lambda x: x.to_numpy()).to_numpy())\n",
    "y = training_data_df['label'].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn with PyTorch\n",
    "\n",
    "- Create a TensorDataset\n",
    "- Create a DataLoader, which shuffles the data\n",
    "- Create a simple neural net (torch.nn.Sequential) which uses CUDA while training\n",
    "- Train the neural net with the data provided\n",
    "- Evaluate the net with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# # Pad sequences to the same length\n",
    "# X_train_padded = pad_sequences(X_train, dtype='float32', padding='post')\n",
    "# y_train_padded = pad_sequences(y_train, dtype='float32', padding='post')\n",
    "# X_test_padded = pad_sequences(X_test, dtype='float32', padding='post')\n",
    "# y_test_padded = pad_sequences(y_test, dtype='float32', padding='post')\n",
    "\n",
    "# Create tensors from the padded data\n",
    "X_train_tensor = torch.tensor(X_train).permute(0, 2, 1)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "X_test_tensor = torch.tensor(X_test).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the CNN architecture\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels, num_filters, kernel_size, pool_size, hidden_units, dropout_rate, device):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, num_filters, kernel_size),\n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(pool_size)\n",
    "        ).to(device)\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(num_filters, num_filters * 2, kernel_size),\n",
    "            nn.BatchNorm1d(num_filters * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(pool_size)\n",
    "        ).to(device)\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(num_filters * 2, num_filters * 4, kernel_size),\n",
    "            nn.BatchNorm1d(num_filters * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(pool_size)\n",
    "        ).to(device)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        conv1_out_size = (chunk_size - kernel_size + 1) // pool_size\n",
    "        conv2_out_size = (conv1_out_size - kernel_size + 1) // pool_size\n",
    "        conv3_out_size = (conv2_out_size - kernel_size + 1) // pool_size\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(num_filters * 4 * conv3_out_size, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ).to(device)\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(hidden_units, hidden_units // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ).to(device)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_units // 2, 1).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_channels = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\illya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01222599926404655\n",
      "Average Absolute Difference: 0.0945901934024507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01222599926404655"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_and_eval_net(trial):\n",
    "    # Suggest hyperparameters using the trial object\n",
    "    # num_filters = trial.suggest_int(\"num_filters\", 128, 512)\n",
    "    # kernel_size = trial.suggest_int(\"kernel_size\", 3, 5)\n",
    "    # pool_size = trial.suggest_int(\"pool_size\", 2, 4)\n",
    "    # hidden_units = trial.suggest_int(\"hidden_units\", 32, 72)\n",
    "    # dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.3)\n",
    "    # learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    \n",
    "    # GOAT (sometimes very good, sometimes bad)\n",
    "    num_filters = 407\n",
    "    kernel_size = 5\n",
    "    pool_size = 3\n",
    "    hidden_units = 49\n",
    "    dropout_rate = 0.14489704854106614\n",
    "    learning_rate = 0.0005565571451646218\n",
    "    \n",
    "    # Value: 0.008048494346439838\n",
    "    # Params: {'num_filters': 435, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 61, 'dropout_rate': 0.1435561530697743, 'learning_rate': 0.0005436599406923652}\n",
    "    # Consistently okay\n",
    "    # num_filters = 435\n",
    "    # kernel_size = 5\n",
    "    # pool_size = 4\n",
    "    # hidden_units = 61\n",
    "    # dropout_rate = 0.1435561530697743\n",
    "    # learning_rate = 0.0005436599406923652\n",
    "    \n",
    "    # Create the model\n",
    "    net = CNNRegressor(input_channels, num_filters, kernel_size, pool_size, hidden_units, dropout_rate, device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    num_epochs = 400\n",
    "    \n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs} Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        total_difference = 0.0\n",
    "        fake_difference = 0.0\n",
    "        num_samples = 0\n",
    "        test_loss = 0.0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Calculate the absolute difference between the predicted and real labels\n",
    "            difference = torch.abs(outputs.squeeze() - labels)\n",
    "\n",
    "            # Update the total difference and the number of samples\n",
    "            total_difference += difference.sum().item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # random outputs\n",
    "            fake_outputs = torch.tensor([random.uniform(0.2, 0.6) for _ in range(len(outputs))], device=device)\n",
    "            fake_diff = torch.abs(fake_outputs - labels)\n",
    "            fake_difference += fake_diff.sum().item()\n",
    "\n",
    "        print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "        # Calculate the average absolute difference\n",
    "        average_difference = total_difference / num_samples\n",
    "        average_fake_difference = fake_difference / num_samples\n",
    "        print(f\"Average Absolute Difference: {average_difference}\")\n",
    "        # print(f\"Average Absolute Fake Difference: {average_fake_difference}\")\n",
    "    \n",
    "    return test_loss/len(test_loader)\n",
    "\n",
    "train_and_eval_net(None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization via Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(train_and_eval_net, n_trials=100)  # You can adjust the number of trials depending on your computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the best 5 trials\n",
    "# completed_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.COMPLETE])\n",
    "# best_trials = sorted(completed_trials, key=lambda t: t.value)[:5]\n",
    "\n",
    "# # Print the best 5 trials' parameters and their respective values\n",
    "# for i, trial in enumerate(best_trials):\n",
    "#     print(f\"Best trial {i + 1}:\")\n",
    "#     print(f\"  Value: {trial.value}\")\n",
    "#     print(f\"  Params: {trial.params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
