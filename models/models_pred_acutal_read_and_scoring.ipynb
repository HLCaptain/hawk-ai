{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn import metrics\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import itertools\n",
        "import csv"
      ],
      "metadata": {
        "id": "YaWadcBWuNlI"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One column\n",
        "# p30_u10 - configuration key: Percentile 30, Uniformity 10\n",
        "# nature - model type: nature, urban or all\n",
        "# all - dataset type: nature, urban or all\n",
        "# 0.6424242854118347 - loss value for picture 0\n",
        "# 15 more float values for 15 more pictures\n",
        "df_losses = pd.read_csv('model_predictions.csv')\n",
        "# One column\n",
        "# p30_u10 - configuration key: Percentile 30, Uniformity 10\n",
        "# all - dataset type: nature, urban or all\n",
        "# 0.6424242854118347 - actual value for picture 0\n",
        "# 15 more float values for 15 more pictures\n",
        "df_actual = pd.read_csv('image_actuals.csv')"
      ],
      "metadata": {
        "id": "hE0DJvksuvo-"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_losses.columns:\n",
        "    #remove generated index from col_name\n",
        "    col_name = re.sub('\\.\\d$', '', col)\n",
        "    # get first and second rows values for this specific column\n",
        "    first_row = df_losses.iloc[0][col]\n",
        "    second_row = df_losses.iloc[1][col]\n",
        "    #new name of this column\n",
        "    new_column_name = f\"{col_name}_{first_row}_{second_row}\"\n",
        "    # rename the column with the existing column header plus the first 2 rows of that column's data\n",
        "    df_losses.rename(columns={col: new_column_name}, inplace=True)\n",
        "\n",
        "# delete first 2 rows of data, now that it's merged into the header\n",
        "df_losses.drop(index=[0,1], inplace=True)\n",
        "df_losses.drop(columns=df_losses.columns[0], axis=1, inplace=True)\n",
        "df_losses.reset_index(drop=True, inplace=True)\n",
        "df_losses=df_losses.astype(float)\n",
        "\n",
        "for col in df_actual.columns:\n",
        "    #remove generated index from col_name\n",
        "    col_name = re.sub('\\.\\d$', '', col)\n",
        "    # get first row value for this specific column\n",
        "    first_row = df_actual.iloc[0][col]\n",
        "    #new name of this column\n",
        "    new_column_name_2 = f\"{col_name}_{first_row}\"\n",
        "    # rename the column with the existing column header plus the first row of that column's data\n",
        "    df_actual.rename(columns={col: new_column_name_2}, inplace=True)\n",
        "\n",
        "# delete first row of data, now that it's merged into the header\n",
        "df_actual.drop(index=[0], inplace=True)\n",
        "df_actual.drop(columns=df_actual.columns[0], axis=1, inplace=True)\n",
        "df_actual.reset_index(drop=True,inplace=True)\n",
        "df_actual=df_actual.astype(float)"
      ],
      "metadata": {
        "id": "YcqkDXTivoRw"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#True classsification, based on the original values, if that > 0.5 new value = 1, else new value = 0\n",
        "df_losses_true = pd.DataFrame(df_losses, index=df_losses.index, columns=df_losses.columns)\n",
        "df_actual_true = pd.DataFrame(df_actual, index=df_actual.index, columns=df_actual.columns)\n",
        "for col in df_losses_true.columns:\n",
        "  df_losses_true.loc[df_losses_true[col] > 0.5, col] = 1\n",
        "  df_losses_true.loc[df_losses_true[col] <= 0.5, col] = 0\n",
        "for col in df_actual_true.columns:\n",
        "  df_actual_true.loc[df_actual_true[col] > 0.5, col] = 1\n",
        "  df_actual_true.loc[df_actual_true[col] <= 0.5, col] = 0"
      ],
      "metadata": {
        "id": "3w3jw1qk2Xhr"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#percent_uniform, model and dataset possible values sets\n",
        "#for model predicted-actual values separation\n",
        "test_type_keys = {\n",
        "    'percent_uniform': set(),\n",
        "    'model': set(),\n",
        "    'dataset': set()\n",
        "}\n",
        "for name in df_losses_true.columns:\n",
        "  type_values = name.split('_')\n",
        "  test_type_keys['percent_uniform'].add(type_values[0]+\"_\"+type_values[1])\n",
        "  test_type_keys['model'].add(type_values[2])\n",
        "  test_type_keys['dataset'].add(type_values[3])\n",
        "for name in df_actual_true.columns:\n",
        "  type_values = name.split('_')\n",
        "  test_type_keys['percent_uniform'].add(type_values[0]+\"_\"+type_values[1])\n",
        "  test_type_keys['dataset'].add(type_values[2])"
      ],
      "metadata": {
        "id": "T_Hj9yMyrXq8"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_2_split(text):\n",
        "  text_splitted = text.split('_')[0:2]\n",
        "  return text_splitted[0]+\"_\"+text_splitted[1]"
      ],
      "metadata": {
        "id": "EMSYQ-Ombotv"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction and actual lists for all configuration and model combination\n",
        "#(Dataset tests are combined)\n",
        "losses_models_names = []\n",
        "actual_image_names = []\n",
        "for f in test_type_keys['model']:\n",
        "  losses_combinations = [f\"{l[0]}_{l[1]}_{l[2]}\" for l in list(itertools.product(test_type_keys['percent_uniform'], [f], test_type_keys['dataset']))]\n",
        "  actuals_combinations = [f\"{l[0]}_{l[2]}\" for l in list(itertools.product(test_type_keys['percent_uniform'], [f], test_type_keys['dataset']))]\n",
        "  for key, group in itertools.groupby(losses_combinations, first_2_split):\n",
        "    losses_models_names.append(np.array(list(group)))\n",
        "  for key, group in itertools.groupby(actuals_combinations, first_2_split):\n",
        "    actual_image_names.append(np.array(list(group)))"
      ],
      "metadata": {
        "id": "k_OcN11ZVlfN"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix(actuals,losses):\n",
        "  tn, fp, fn, tp  = metrics.confusion_matrix(actuals,losses).ravel() #roc_curve(test, pred, drop_intermediate=False)\n",
        "  return tn, fp, fn, tp"
      ],
      "metadata": {
        "id": "FM-Ioq-log0R"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Intersection over Union\n",
        "def IoU_score(tn, fp, fn, tp):\n",
        "  IoU = tp / (tp+fn+fp)\n",
        "  return IoU"
      ],
      "metadata": {
        "id": "jMtgYdgE2VBm"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sørensen–Dice\n",
        "def DSC_score(tn, fp, fn, tp):\n",
        "  DSC = (2*tp) / (2*tp+fn+fp)\n",
        "  return DSC"
      ],
      "metadata": {
        "id": "-VeAFfVt23lx"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scores of the models\n",
        "score_table=[]\n",
        "for i in range(min(len(losses_models_names),len(actual_image_names))):\n",
        "  losses = np.array(df_losses_true[losses_models_names[i]]).reshape(16*losses_models_names[i].size)\n",
        "  actuals = np.array(df_actual_true[actual_image_names[i]]).reshape(16*actual_image_names[i].size)\n",
        "\n",
        "  model_name_sep = df_losses_true[losses_models_names[i]].columns[0].split('_')\n",
        "  model_name = model_name_sep[0]+\"_\"+model_name_sep[1]+\"_\"+model_name_sep[2]\n",
        "  tn, fp, fn, tp=confusion_matrix(actuals,losses)\n",
        "  IoU = IoU_score(tn, fp, fn, tp)\n",
        "  DSC = DSC_score(tn, fp, fn, tp)\n",
        "  score_table.append({'name':model_name,\n",
        "                      'tn': tn, 'fp': fp,\n",
        "                      'fn': fn, 'tp': tp,\n",
        "                      'IoU': IoU, 'DSC': DSC})\n",
        "print(score_table)"
      ],
      "metadata": {
        "id": "c8-s54PWhUdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912473ff-d88c-4815-9f5c-1c9a2c463515"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'p70_u25_urban', 'tn': 0, 'fp': 8, 'fn': 0, 'tp': 40, 'IoU': 0.8333333333333334, 'DSC': 0.9090909090909091}, {'name': 'p50_u40_urban', 'tn': 0, 'fp': 11, 'fn': 0, 'tp': 37, 'IoU': 0.7708333333333334, 'DSC': 0.8705882352941177}, {'name': 'p30_u25_urban', 'tn': 0, 'fp': 13, 'fn': 0, 'tp': 35, 'IoU': 0.7291666666666666, 'DSC': 0.8433734939759037}, {'name': 'p70_u40_urban', 'tn': 0, 'fp': 8, 'fn': 0, 'tp': 40, 'IoU': 0.8333333333333334, 'DSC': 0.9090909090909091}, {'name': 'p50_u10_urban', 'tn': 0, 'fp': 7, 'fn': 0, 'tp': 41, 'IoU': 0.8541666666666666, 'DSC': 0.9213483146067416}, {'name': 'p30_u10_urban', 'tn': 0, 'fp': 9, 'fn': 0, 'tp': 39, 'IoU': 0.8125, 'DSC': 0.896551724137931}, {'name': 'p50_u25_urban', 'tn': 0, 'fp': 11, 'fn': 0, 'tp': 37, 'IoU': 0.7708333333333334, 'DSC': 0.8705882352941177}, {'name': 'p70_u10_urban', 'tn': 5, 'fp': 4, 'fn': 2, 'tp': 37, 'IoU': 0.8604651162790697, 'DSC': 0.925}, {'name': 'p30_u40_urban', 'tn': 0, 'fp': 13, 'fn': 0, 'tp': 35, 'IoU': 0.7291666666666666, 'DSC': 0.8433734939759037}, {'name': 'p70_u25_all', 'tn': 0, 'fp': 8, 'fn': 0, 'tp': 40, 'IoU': 0.8333333333333334, 'DSC': 0.9090909090909091}, {'name': 'p50_u40_all', 'tn': 0, 'fp': 11, 'fn': 0, 'tp': 37, 'IoU': 0.7708333333333334, 'DSC': 0.8705882352941177}, {'name': 'p30_u25_all', 'tn': 0, 'fp': 13, 'fn': 0, 'tp': 35, 'IoU': 0.7291666666666666, 'DSC': 0.8433734939759037}, {'name': 'p70_u40_all', 'tn': 0, 'fp': 8, 'fn': 0, 'tp': 40, 'IoU': 0.8333333333333334, 'DSC': 0.9090909090909091}, {'name': 'p50_u10_all', 'tn': 0, 'fp': 7, 'fn': 0, 'tp': 41, 'IoU': 0.8541666666666666, 'DSC': 0.9213483146067416}, {'name': 'p30_u10_all', 'tn': 0, 'fp': 9, 'fn': 0, 'tp': 39, 'IoU': 0.8125, 'DSC': 0.896551724137931}, {'name': 'p50_u25_all', 'tn': 0, 'fp': 11, 'fn': 0, 'tp': 37, 'IoU': 0.7708333333333334, 'DSC': 0.8705882352941177}, {'name': 'p70_u10_all', 'tn': 0, 'fp': 9, 'fn': 0, 'tp': 39, 'IoU': 0.8125, 'DSC': 0.896551724137931}, {'name': 'p30_u40_all', 'tn': 0, 'fp': 13, 'fn': 0, 'tp': 35, 'IoU': 0.7291666666666666, 'DSC': 0.8433734939759037}, {'name': 'p70_u25_nature', 'tn': 0, 'fp': 8, 'fn': 0, 'tp': 40, 'IoU': 0.8333333333333334, 'DSC': 0.9090909090909091}, {'name': 'p50_u40_nature', 'tn': 0, 'fp': 11, 'fn': 0, 'tp': 37, 'IoU': 0.7708333333333334, 'DSC': 0.8705882352941177}, {'name': 'p30_u25_nature', 'tn': 0, 'fp': 13, 'fn': 0, 'tp': 35, 'IoU': 0.7291666666666666, 'DSC': 0.8433734939759037}, {'name': 'p70_u40_nature', 'tn': 0, 'fp': 8, 'fn': 0, 'tp': 40, 'IoU': 0.8333333333333334, 'DSC': 0.9090909090909091}, {'name': 'p50_u10_nature', 'tn': 0, 'fp': 7, 'fn': 0, 'tp': 41, 'IoU': 0.8541666666666666, 'DSC': 0.9213483146067416}, {'name': 'p30_u10_nature', 'tn': 0, 'fp': 9, 'fn': 0, 'tp': 39, 'IoU': 0.8125, 'DSC': 0.896551724137931}, {'name': 'p50_u25_nature', 'tn': 0, 'fp': 11, 'fn': 0, 'tp': 37, 'IoU': 0.7708333333333334, 'DSC': 0.8705882352941177}, {'name': 'p70_u10_nature', 'tn': 5, 'fp': 4, 'fn': 2, 'tp': 37, 'IoU': 0.8604651162790697, 'DSC': 0.925}, {'name': 'p30_u40_nature', 'tn': 0, 'fp': 13, 'fn': 0, 'tp': 35, 'IoU': 0.7291666666666666, 'DSC': 0.8433734939759037}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_table_fields = ['name', 'tn', 'fp', 'fn', 'tp', 'IoU', 'DSC']\n",
        "filename = \"model_scores.csv\"\n",
        "with open(filename, 'w') as csvfile:\n",
        "    # creating a csv dict writer object\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=score_table_fields)\n",
        "\n",
        "    # writing headers (field names)\n",
        "    writer.writeheader()\n",
        "\n",
        "    # writing data rows\n",
        "    writer.writerows(score_table)"
      ],
      "metadata": {
        "id": "f-p0jeMxrXDD"
      },
      "execution_count": 179,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
