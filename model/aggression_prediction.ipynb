{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Aggression Neural Network (DANN)\n",
    "\n",
    "Driver Aggression Neural Network is assigning an aggression value to a sorted set of sensory data. Driving is simulated in BeamNG v0.27 using their BeamNGpy open-source library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parquet_file_path = '../imu_data_2023_05_03_13_16_18.parquet'\n",
    "data = pd.read_parquet(parquet_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify training data\n",
    "\n",
    "- Group together data recorded from the same sensor\n",
    "- Take around 100-1000 recorded data without the aggression values\n",
    "- Make aggression values the label of the dataset\n",
    "- Create a lot of training data by chunking the sorted (by timestamp) records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time     dirXX     dirXY     dirXZ     dirYX     dirYY     dirYZ     dirZX     dirZY     dirZZ    accRawX    accRawY    accRawZ  accSmoothX  accSmoothY  accSmoothZ   angVelX   angVelY   angVelZ  angAccelX  angAccelY  angAccelZ\n",
      "0    0.00000 -0.708555 -0.703101  0.059992 -0.001492 -0.083523 -0.996505  0.705654 -0.706168  0.058132  -2.000044   0.455348   1.687304   -1.434216    0.180359    1.261864  0.019011 -0.009269 -0.006625  -1.862096   0.423417   6.167933\n",
      "1    0.01025 -0.708555 -0.703101  0.059992 -0.001492 -0.083523 -0.996505  0.705654 -0.706168  0.058132  -0.694419  -0.174066   1.633643   -0.842604   -0.103073    1.559174  0.009397 -0.004004 -0.002393  -1.326108   1.985857  -0.100370\n",
      "2    0.02050 -0.708593 -0.703074  0.059857 -0.001539 -0.083289 -0.996524  0.705615 -0.706223  0.057936  -0.248124  -0.637252  -5.505297   -0.367201   -0.530253   -4.090256  0.012850  0.000724 -0.009720   0.026034   0.057339  -5.679400\n",
      "3    0.03075 -0.708820 -0.702860  0.059679 -0.001632 -0.082970 -0.996551  0.705387 -0.706473  0.057663  -1.061653   0.943569   2.411685   -0.922551    0.648356    1.109321  0.004505  0.000527 -0.009149  -0.129931  -0.877236   3.226284\n",
      "4    0.04100 -0.708820 -0.702860  0.059679 -0.001632 -0.082970 -0.996551  0.705387 -0.706473  0.057663  -0.185279  -0.958888   1.675305   -0.332957   -0.636951    1.561936  0.002714 -0.003015 -0.012027   0.968395  -1.528812   3.983624\n",
      "5    0.05125 -0.708938 -0.702749  0.059584 -0.001559 -0.082922 -0.996555  0.705269 -0.706589  0.057691   0.912365  -1.812507  -0.639859    0.662922   -1.577038   -0.198831  0.010502 -0.002577 -0.011250  -0.432412  -0.847619  -0.420335\n",
      "6    0.06150 -0.708938 -0.702749  0.059584 -0.001559 -0.082922 -0.996555  0.705269 -0.706589  0.057691   0.357214  -1.304496  -0.184435    0.418449   -1.359087   -0.187318 -0.000108 -0.007883 -0.019535  -2.737271   0.835781  -1.002638\n",
      "7    0.07175 -0.709154 -0.702541  0.059463 -0.001494 -0.082840 -0.996562  0.705052 -0.706805  0.057697  -0.625937   2.531236  -0.205789   -0.416743    1.751989   -0.202089 -0.004257 -0.003521 -0.015667   0.269491  -0.841857  -2.186483\n",
      "8    0.08200 -0.709359 -0.702337  0.059433 -0.001313 -0.083004 -0.996548  0.704846 -0.706988  0.057957   0.615445  -3.735191   0.543901    0.408693   -2.636088    0.394476  0.000616 -0.003218 -0.017851  -4.721224   3.003138  -0.320370\n",
      "9    0.09225 -0.709359 -0.702337  0.059433 -0.001313 -0.083004 -0.996548  0.704846 -0.706988  0.057957  -0.183007   4.722112  -1.000782   -0.064487    3.248236   -0.721307 -0.009451 -0.000350 -0.012346   3.656813  -2.266086  -1.224583\n",
      "10   0.10250 -0.709545 -0.702151  0.059409 -0.001056 -0.083248 -0.996528  0.704659 -0.707145  0.058327   1.086492  -3.284717  -1.513241    0.855946   -1.976141   -1.354614 -0.016774  0.001206 -0.016119  -4.111549   1.580810  -2.642704\n",
      "11   0.11275 -0.709545 -0.702151  0.059409 -0.001056 -0.083248 -0.996528  0.704659 -0.707145  0.058327   1.073557  -0.193677   0.546280    1.029969   -0.550712    0.165524 -0.002819 -0.004003 -0.007214   1.016017  -0.358321  -1.057140\n",
      "12   0.12300 -0.709708 -0.701982  0.059467 -0.000811 -0.083597 -0.996499  0.704496 -0.707272  0.058760   0.648836   2.879482   0.995587    0.725178    2.192401    0.829322 -0.019600  0.002552 -0.007430   3.370628  -1.408127   1.947421\n",
      "13   0.13325 -0.709816 -0.701867  0.059530 -0.000575 -0.083936 -0.996471  0.704387 -0.707345  0.059175   0.304129  -3.651573  -0.113819    0.388467   -2.481002    0.075095 -0.015492  0.002459 -0.004522  -4.785021   2.204950  -0.545631\n",
      "14   0.14350 -0.709816 -0.701867  0.059530 -0.000575 -0.083936 -0.996471  0.704387 -0.707345  0.059175   0.870979   2.077974   0.430047    0.774330    1.164794    0.358949 -0.006744  0.001017  0.000650   2.710344  -1.415754   0.764951\n",
      "15   0.15375 -0.709864 -0.701804  0.059701 -0.000484 -0.084275 -0.996442  0.704338 -0.707368  0.059484   1.118164  -1.247298   0.916936    1.049293   -0.764147    0.805169 -0.020005  0.006411 -0.000704  -0.021533   0.177771   1.047594\n",
      "16   0.16400 -0.709837 -0.701811  0.059937 -0.000502 -0.084589 -0.996416  0.704365 -0.707323  0.059692   1.278329  -0.310682  -0.395966    1.232452   -0.401513   -0.155374 -0.011824  0.004404  0.002210  -0.916612   0.351141  -1.662392\n",
      "17   0.17425 -0.709837 -0.701811  0.059937 -0.000502 -0.084589 -0.996416  0.704365 -0.707323  0.059692   0.877505   0.196590   1.141585    0.948603    0.076788    0.881799 -0.012711  0.005698  0.003129   0.603527   0.141330   1.280033\n",
      "18   0.18450 -0.709817 -0.701804  0.060248 -0.000695 -0.084835 -0.996395  0.704385 -0.707300  0.059730   1.008659  -1.172241   0.000982    0.996630   -0.922056    0.177413 -0.014214  0.008633  0.005257  -1.674104   1.127891   0.006253\n",
      "19   0.19475 -0.709817 -0.701804  0.060248 -0.000695 -0.084835 -0.996395  0.704385 -0.707300  0.059730   1.548736  -0.454822   0.680759    1.438147   -0.548411    0.579936 -0.010484  0.007669  0.006646   0.018416  -0.250329  -0.003295\n",
      "20   0.20500 -0.709734 -0.701858  0.060597 -0.001009 -0.085006 -0.996380  0.704469 -0.707226  0.059623   1.373642  -0.102367   0.551897    1.386563   -0.191711    0.557513 -0.012121  0.009076  0.003403   0.931122  -0.681900   0.588343\n",
      "21   0.21525 -0.709667 -0.701891  0.061003 -0.001442 -0.085139 -0.996368  0.704535 -0.707178  0.059409   2.170842   0.003048   0.642607    2.013748   -0.035963    0.625563 -0.009599  0.008850  0.000292   0.430627  -0.125049  -0.225110\n",
      "22   0.22550 -0.709667 -0.701891  0.061003 -0.001442 -0.085139 -0.996368  0.704535 -0.707178  0.059409   1.519815  -0.164616  -0.378367    1.618752   -0.138846   -0.177276 -0.012013  0.010464 -0.004143   0.488125  -0.406514  -0.652517\n",
      "23   0.23575 -0.709685 -0.701834  0.061452 -0.002008 -0.085210 -0.996361  0.704516 -0.707226  0.059063   1.555670  -0.637443   0.350952    1.568305   -0.537572    0.245146 -0.012821  0.011004 -0.002526  -0.859442   0.305898  -0.350198\n",
      "24   0.24600 -0.709685 -0.701834  0.061452 -0.002008 -0.085210 -0.996361  0.704516 -0.707226  0.059063   6.793842   1.440449   1.809795    5.747147    1.044244    1.496390  0.001350  0.003497 -0.006599   4.072890  -1.819692  -1.809962\n",
      "25   0.25625 -0.709692 -0.701787  0.061911 -0.002577 -0.085291 -0.996353  0.704508 -0.707263  0.058722   0.952532  -4.257098   0.353546    1.912912   -3.195219    0.582462 -0.030212  0.022136 -0.015210  -5.012981   1.950024   0.330663\n",
      "26   0.26650 -0.709803 -0.701606  0.062679 -0.003530 -0.085438 -0.996337  0.704392 -0.707424  0.058167   4.119627   2.053176  -0.946619    3.677613    1.001902   -0.640338 -0.008961  0.012155 -0.030930   3.066049  -0.769428  -3.687044\n",
      "27   0.27675 -0.709803 -0.701606  0.062679 -0.003530 -0.085438 -0.996337  0.704392 -0.707424  0.058167   3.696910  -3.639354   2.077531    3.693045   -2.709693    1.533132 -0.005361  0.011196 -0.039561  -3.516836   1.742497  -1.152501\n",
      "28   0.28700 -0.710219 -0.701127  0.063318 -0.004562 -0.085356 -0.996340  0.703966 -0.707909  0.057423   2.798874   4.187919   0.573569    2.977980    2.806301    0.765773 -0.044798  0.028789 -0.052355   5.864858  -2.334030   2.136709\n",
      "29   0.29725 -0.710683 -0.700552  0.064477 -0.005945 -0.085667 -0.996306  0.703487 -0.708441  0.056718   2.876663  -4.132893  -1.233374    2.896957   -2.742945   -0.832937 -0.026842  0.020853 -0.051003  -4.841216   2.414305  -2.575011\n",
      "30   0.30750 -0.710683 -0.700552  0.064477 -0.005945 -0.085667 -0.996306  0.703487 -0.708441  0.056718   2.753777   1.175343   2.649309    2.782457    0.390494    1.951802 -0.031435  0.021229 -0.047698   1.376454  -0.520735   1.200144\n",
      "31   0.31775 -0.711216 -0.699899  0.065675 -0.007280 -0.086085 -0.996261  0.702936 -0.709035  0.056130   4.702534  -1.303771  -1.928423    4.317935   -0.964403   -1.151199 -0.026409  0.025266 -0.070694   0.770741  -0.365060  -5.324901\n",
      "32   0.32800 -0.711216 -0.699899  0.065675 -0.007280 -0.086085 -0.996261  0.702936 -0.709035  0.056130   1.711236  -0.677291   0.608574    2.233368   -0.734800    0.256084 -0.022769  0.017497 -0.077251  -2.658115   0.771635   0.363062\n",
      "33   0.33825 -0.712037 -0.698969  0.066676 -0.008654 -0.086217 -0.996239  0.702089 -0.709936  0.055341  -0.486693  -3.221646  -1.549701    0.058146   -2.723521   -1.187995 -0.023534  0.015949 -0.087795  -4.887342   2.597682   0.201863\n",
      "34   0.34850 -0.712999 -0.697888  0.067715 -0.009855 -0.086591 -0.996195  0.701096 -0.710953  0.054862   5.539662   7.325514   0.060357    4.441693    5.312653   -0.189693 -0.013788  0.007230 -0.098649  10.575700  -4.585384  -1.379529\n",
      "35   0.35875 -0.712999 -0.697888  0.067715 -0.009855 -0.086591 -0.996195  0.701096 -0.710953  0.054862   0.171064  -8.676434  -0.276843    1.026487   -5.874365   -0.259386 -0.047631  0.024750 -0.085543  -9.430213   3.398792   1.546014\n",
      "36   0.36900 -0.714082 -0.696713  0.068390 -0.010518 -0.087004 -0.996152  0.699983 -0.712054  0.054800   4.823450   0.130738   5.860101    4.062904   -1.072107    4.634344 -0.047079  0.019189 -0.061485   7.343395  -3.785563   4.743018\n",
      "37   0.37925 -0.714082 -0.696713  0.068390 -0.010518 -0.087004 -0.996152  0.699983 -0.712054  0.054800   6.968520  -8.138996  -3.525180    6.386514   -6.723471   -1.890795  0.000762 -0.002110 -0.081834  -7.338659   3.064560  -0.911519\n",
      "38   0.38950 -0.714857 -0.695857  0.069010 -0.011006 -0.087480 -0.996106  0.699184 -0.712832  0.054876   5.932548   4.902663   0.580568    6.023479    2.573904    0.085544 -0.046578  0.012037 -0.071870   5.165312  -1.468976  -3.054473\n",
      "39   0.39975 -0.715718 -0.694918  0.069555 -0.011304 -0.088054 -0.996052  0.698298 -0.713678  0.055167  -1.055524  -5.611652   3.381841    0.362427   -3.972053    2.721580 -0.016568  0.007271 -0.063074  -3.970838   2.944854   5.328232\n",
      "40   0.41000 -0.715718 -0.694918  0.069555 -0.011304 -0.088054 -0.996052  0.698298 -0.713678  0.055167   8.048566   3.926640  -0.610703    6.509002    2.344501    0.056766  0.018402 -0.003538 -0.058019  11.073288  -5.978715  -1.607873\n",
      "41   0.42025 -0.716412 -0.694203  0.069550 -0.011408 -0.088019 -0.996054  0.697585 -0.714378  0.055138   3.473830  -0.040891  -4.220229    4.081787    0.436913   -3.363530 -0.009240 -0.000515 -0.061998   2.678861  -3.134083  -1.127971\n",
      "42   0.43050 -0.716945 -0.693654  0.069525 -0.011589 -0.087858 -0.996066  0.697033 -0.714930  0.054951   2.365573  -0.704714   2.413245    2.709337   -0.476042    1.256135  0.044991 -0.023122 -0.020999   0.528954  -0.737758   1.891158\n",
      "43   0.44075 -0.716945 -0.693654  0.069525 -0.011589 -0.087858 -0.996066  0.697033 -0.714930  0.054951  -0.154552   0.206154  -1.434148    0.419096    0.069507   -0.895274  0.040432 -0.017475 -0.030839  -1.685556   2.657035  -2.001658\n",
      "44   0.45100 -0.717316 -0.693314  0.069085 -0.011592 -0.087265 -0.996118  0.696652 -0.715332  0.054560  -0.959333  -2.198719   3.752686   -0.683228   -1.744385    2.821681  0.005629  0.000885 -0.017150  -5.782470   3.882842   2.736527\n",
      "45   0.46125 -0.717316 -0.693314  0.069085 -0.011592 -0.087265 -0.996118  0.696652 -0.715332  0.054560   4.579457   1.123091  -2.191863    3.525321    0.548724   -1.187631  0.035105 -0.011741 -0.026860   4.243828  -1.956238  -3.231825\n",
      "46   0.47150 -0.717648 -0.692999  0.068808 -0.011571 -0.086926 -0.996148  0.696310 -0.715679  0.054364   3.946392  -2.791635   4.157234    3.862050   -2.122548    3.086637  0.036365 -0.016722 -0.025925  -0.489223   0.427095   0.279324\n",
      "47   0.48175 -0.717953 -0.692732  0.068310 -0.011426 -0.086393 -0.996196  0.695998 -0.716002  0.054112   1.114190   4.277823   5.272351    1.664597    2.995804    4.834544  0.007121 -0.006356 -0.023843   7.848853  -2.190485   7.517041\n",
      "48   0.49200 -0.717953 -0.692732  0.068310 -0.011426 -0.086393 -0.996196  0.695998 -0.716002  0.054112   0.156377  -4.238027   1.192488    0.458479   -2.789062    1.922006  0.027796 -0.007381 -0.025994  -4.953304   3.301419   2.850836\n",
      "49   0.50225 -0.718251 -0.692469  0.067839 -0.011320 -0.085856 -0.996243  0.695692 -0.716320  0.053828   2.884778   0.215120   0.600900    2.398781   -0.386630    0.865522  0.044321 -0.016769 -0.028363   2.259078  -1.786737   0.445730\n",
      "50   0.51250 -0.718251 -0.692469  0.067839 -0.011320 -0.085856 -0.996243  0.695692 -0.716320  0.053828   1.321748  -1.122498  -0.604245    1.537482   -0.975101   -0.309845  0.017798 -0.013384 -0.012628   0.572244  -0.445641  -0.940569\n",
      "51   0.52275 -0.718524 -0.692252  0.067162 -0.011003 -0.085239 -0.996300  0.695415 -0.716604  0.053630   0.795074   1.194169  -0.611822    0.943781    0.759656   -0.551335  0.021152 -0.016173 -0.014589   0.370536   0.367744  -0.723414\n",
      "52   0.53300 -0.718690 -0.692137  0.066558 -0.010329 -0.085084 -0.996320  0.695253 -0.716733  0.054000   2.254234   3.250460   2.676935    1.991745    2.751542    2.030300  0.015499 -0.016712 -0.024282   2.025823   0.049138   2.555442\n",
      "53   0.54325 -0.718690 -0.692137  0.066558 -0.010329 -0.085084 -0.996320  0.695253 -0.716733  0.054000   2.706859  -1.668340   0.359328    2.563619   -0.783020    0.694030  0.006491 -0.012088 -0.040207  -1.494659   0.787608  -0.998244\n",
      "54   0.55350 -0.719102 -0.691769  0.065939 -0.009507 -0.085087 -0.996328  0.694839 -0.717089  0.054609   2.276037   3.498832   0.392010    2.333641    2.641161    0.452505  0.013792 -0.016160 -0.037175   3.021244  -1.600143   0.125227\n",
      "55   0.56375 -0.719604 -0.691305  0.065336 -0.008638 -0.085172 -0.996329  0.694331 -0.717526  0.055318   1.345005  -1.131747  -0.571032    1.543033   -0.376019   -0.366013 -0.007055 -0.009081 -0.037628  -1.358314   1.286181  -0.753496\n",
      "56   0.57400 -0.719604 -0.691305  0.065336 -0.008638 -0.085172 -0.996329  0.694331 -0.717526  0.055318   3.535722   1.409738   1.768476    3.136579    1.052044    1.340929 -0.011650 -0.003330 -0.032575   3.116731  -1.850746  -1.765341\n",
      "57   0.58425 -0.720018 -0.690899  0.065063 -0.007949 -0.085540 -0.996303  0.693910 -0.717873  0.056098   2.770823  -1.944106   0.156288    2.844086   -1.343965    0.393576  0.007634 -0.007101 -0.053898  -1.319460   0.713058  -0.444512\n",
      "58   0.59450 -0.720018 -0.690899  0.065063 -0.007949 -0.085540 -0.996303  0.693910 -0.717873  0.056098   2.565473   5.379574   2.743653    2.621280    4.032823    2.272923 -0.007031 -0.000895 -0.047619   6.789375  -3.514296   1.255698\n",
      "59   0.60475 -0.720619 -0.690287  0.064906 -0.007753 -0.085586 -0.996301  0.693288 -0.718456  0.056323   2.675590  -4.946866  -2.436064    2.664711   -3.148199   -1.492835 -0.000900  0.004465 -0.063568  -5.513109   2.292722  -4.037717\n",
      "60   0.61500 -0.721313 -0.689561  0.064906 -0.008007 -0.085405 -0.996314  0.692563 -0.719174  0.056082   1.794485   0.911213   1.797645    1.968795    0.098097    1.138549  0.010526 -0.001185 -0.062293   1.163445  -0.154261   0.425500\n",
      "61   0.62525 -0.721313 -0.689561  0.064906 -0.008007 -0.085405 -0.996314  0.692563 -0.719174  0.056082   3.149404  -1.339651   1.867013    2.912924   -1.051664    1.721099  0.012948 -0.000600 -0.059135  -1.898149  -0.036226  -0.942550\n",
      "62   0.63550 -0.722002 -0.688841  0.064900 -0.008431 -0.085035 -0.996342  0.691840 -0.719908  0.055587   4.463618  -0.752966   0.086200    4.153008   -0.812796    0.413677  0.025539 -0.000162 -0.055648   1.369305  -0.954451  -1.745385\n",
      "63   0.64575 -0.722002 -0.688841  0.064900 -0.008431 -0.085035 -0.996342  0.691840 -0.719908  0.055587   2.495785   2.428363  -3.001710    2.827733    1.779146   -2.317595  0.000317  0.008883 -0.055442   0.267810  -0.210548  -3.626879\n",
      "64   0.65600 -0.722600 -0.688192  0.065127 -0.009336 -0.084489 -0.996381  0.691203 -0.720593  0.054626   2.477186  -1.871676  -2.200412    2.547402   -1.140402   -2.223884  0.002395  0.003313 -0.073137  -5.417798   2.841699  -2.907989\n",
      "65   0.66625 -0.723397 -0.687329  0.065386 -0.009938 -0.084327 -0.996389  0.690361 -0.721434  0.054171   3.534228   0.038890  -1.546798    3.336563   -0.197327   -1.682421 -0.010660  0.007654 -0.067575   1.112639  -0.645377  -2.267185\n",
      "66   0.67650 -0.723397 -0.687329  0.065386 -0.009938 -0.084327 -0.996389  0.690361 -0.721434  0.054171   2.526111  -1.730060   2.386350    2.688448   -1.423047    1.571360 -0.027414  0.015751 -0.059944  -2.140780   0.364251   3.072454\n",
      "67   0.68675 -0.724092 -0.686551  0.065865 -0.010466 -0.084549 -0.996364  0.689624 -0.722149  0.054036   3.196485   0.049425  -4.676548    3.094723   -0.245517   -3.425068 -0.040401  0.014841 -0.085330   1.327702  -1.400999  -5.821326\n",
      "68   0.69700 -0.725134 -0.685378  0.066614 -0.010961 -0.085236 -0.996300  0.688520 -0.723182  0.054295   3.382372  -1.968963  -0.047150    3.324755   -1.623750   -0.723760 -0.023520  0.003696 -0.082756  -1.261340   0.111329   0.785426\n",
      "69   0.70725 -0.725134 -0.685378  0.066614 -0.010961 -0.085236 -0.996300  0.688520 -0.723182  0.054295   3.788021  -0.929056   5.781401    3.695227   -1.068206    4.478392 -0.039628  0.012728 -0.056394   0.420801   0.713543   3.755265\n",
      "70   0.71750 -0.725810 -0.684586  0.067393 -0.011360 -0.086028 -0.996228  0.687801 -0.723838  0.054664   2.067577  -3.677614  -4.054320    2.393602   -3.154939   -2.345185 -0.044557  0.017435 -0.066163  -4.575666   3.781957  -7.252325\n",
      "71   0.72775 -0.725810 -0.684586  0.067393 -0.011360 -0.086028 -0.996228  0.687801 -0.723838  0.054664   2.202469   0.113038  -3.029878    2.240754   -0.541551   -2.892731 -0.013067  0.006834 -0.076310  -3.952911   3.236485  -4.975433\n",
      "72   0.73800 -0.726564 -0.683712  0.068132 -0.011948 -0.086572 -0.996174  0.686995 -0.724598  0.054731   0.959330  -2.820990   0.849801    1.216004   -2.364409    0.100158 -0.051516  0.018949 -0.081711  -0.504631   1.158733   1.185467\n",
      "73   0.74825 -0.727490 -0.682640  0.069007 -0.012314 -0.087569 -0.996082  0.686008 -0.725489  0.055299   3.784491   2.007891  -1.413004    3.270013    1.132103   -1.109912 -0.061575  0.019364 -0.091183   3.021616  -1.368965  -1.324666\n",
      "74   0.75850 -0.727490 -0.682640  0.069007 -0.012314 -0.087569 -0.996082  0.686008 -0.725489  0.055299   1.618496  -2.403490   0.087565    1.949301   -1.695297   -0.152294 -0.053343  0.014165 -0.089196  -1.411237   0.986509  -1.126379\n",
      "75   0.76875 -0.728581 -0.681381  0.069928 -0.012551 -0.088794 -0.995971  0.684845 -0.726523  0.056142   5.677621   1.593622   5.006148    4.930824    0.934839    3.972892 -0.061834  0.019054 -0.076289   5.792888  -2.544454   2.719570\n",
      "76   0.77900 -0.728581 -0.681381  0.069928 -0.012551 -0.088794 -0.995971  0.684845 -0.726523  0.056142   3.122319  -1.225515  -1.564984    3.484570   -0.792788   -0.455726 -0.069942  0.024254 -0.098961   0.127674  -0.648575  -2.287519\n",
      "77   0.78925 -0.729432 -0.680375  0.070847 -0.012743 -0.090035 -0.995857  0.683935 -0.727313  0.057005   7.082669  -7.042178  -3.038095    6.361956   -5.790401   -2.520836 -0.025565  0.004183 -0.085392  -6.717758   2.182578  -5.885313\n",
      "78   0.79950 -0.730538 -0.679106  0.071614 -0.013127 -0.090886 -0.995775  0.682745 -0.728392  0.057481   1.560881   0.995493   1.036555    2.522555   -0.363748    0.323996 -0.049455  0.012169 -0.090288   3.229452  -1.459516   2.055709\n",
      "79   0.80975 -0.730538 -0.679106  0.071614 -0.013127 -0.090886 -0.995775  0.682745 -0.728392  0.057481   4.163259   1.117885  -3.801368    3.834620    0.821108   -2.975042 -0.035161  0.003539 -0.096616  -0.796054   0.902303  -5.274221\n",
      "80   0.82000 -0.731593 -0.677907  0.072215 -0.013140 -0.091886 -0.995683  0.681615 -0.729383  0.058316  -0.652223  -4.250282  -1.558099    0.246509   -3.234463   -1.841918 -0.052818  0.013362 -0.089556  -6.528597   3.725205  -1.563976\n",
      "81   0.83025 -0.732447 -0.676913  0.072867 -0.013116 -0.092978 -0.995582  0.680697 -0.730167  0.059224   7.107955  -1.227920  -0.076242    5.733581   -1.629839   -0.429914 -0.038899  0.009236 -0.059571   3.726520  -4.150515  -7.788321\n",
      "82   0.84050 -0.732447 -0.676913  0.072867 -0.013116 -0.092978 -0.995582  0.680697 -0.730167  0.059224   1.699897   0.647202  -2.986672    2.507859    0.191102   -2.474543  0.001833 -0.005485 -0.109183  -0.981825   1.112740  -1.337112\n",
      "83   0.85075 -0.733568 -0.675665  0.073172 -0.013328 -0.093344 -0.995545  0.679485 -0.731275  0.059469   1.869845  -2.209772   6.193092    1.997642   -1.728868    4.456931 -0.053540  0.013824 -0.077793   0.089161   0.747849   2.389657\n",
      "84   0.86100 -0.733568 -0.675665  0.073172 -0.013328 -0.093344 -0.995545  0.679485 -0.731275  0.059469   1.240290  -8.080637  -3.395183    1.391991   -6.808353   -1.822374  0.000853  0.000855 -0.090193  -7.709718   4.232849  -2.639273\n",
      "85   0.87125 -0.734544 -0.674573  0.073462 -0.013447 -0.093769 -0.995503  0.678428 -0.732229  0.059807   4.709039   4.258224  -1.791439    4.044621    2.041546   -1.797636 -0.020720  0.000397 -0.068219   3.278380  -3.350143 -10.172412\n",
      "86   0.88150 -0.735349 -0.673672  0.073672 -0.013350 -0.094290 -0.995455  0.677557 -0.732991  0.060342   0.232369  -3.494582  -4.367792    0.995978   -2.385674   -3.852980 -0.025067 -0.001503 -0.080329  -4.405215   1.450499  -2.233674\n",
      "87   0.89175 -0.735349 -0.673672  0.073672 -0.013350 -0.094290 -0.995455  0.677557 -0.732991  0.060342   4.992893   4.091100   0.730039    4.192296    2.793777   -0.187957 -0.010785  0.001420 -0.072866   7.754070  -3.956652  -4.244956\n",
      "88   0.90200 -0.736223 -0.672697  0.073855 -0.013209 -0.094830 -0.995406  0.676610 -0.733817  0.060931   3.174986  -4.340069  -2.680061    3.378757   -2.911132   -2.180883 -0.021375  0.004686 -0.091254  -2.067075   0.013325  -3.828390\n",
      "89   0.91225 -0.736223 -0.672697  0.073855 -0.013209 -0.094830 -0.995406  0.676610 -0.733817  0.060931   2.081900   2.866224  -0.266990    2.341665    1.708997   -0.650350 -0.012899 -0.006654 -0.080384   5.066294  -1.628261   5.334565\n",
      "90   0.92250 -0.737272 -0.671537  0.073944 -0.012973 -0.095357 -0.995359  0.675471 -0.734810  0.061593   2.568874   1.993072   3.518602    2.523363    1.936171    2.683544 -0.025495  0.005282 -0.049008   1.591597   0.898882   1.859634\n",
      "91   0.93275 -0.737837 -0.670895  0.074138 -0.012883 -0.095820 -0.995315  0.674856 -0.735335  0.062057   4.273836   3.456569   0.578994    3.923210    3.152027    1.000544 -0.013917 -0.000390 -0.077080   2.683829  -0.767744  -1.798399\n",
      "92   0.94300 -0.737837 -0.670895  0.074138 -0.012883 -0.095820 -0.995315  0.674856 -0.735335  0.062057   1.232994  -2.031732  -2.362775    1.771855   -0.993405   -1.689089 -0.021742  0.004473 -0.064345  -5.977345   3.682704   0.507595\n",
      "93   0.95325 -0.738641 -0.670006  0.074177 -0.012555 -0.096346 -0.995269  0.673982 -0.736077  0.062753   4.364407  -0.739252   1.303899    3.845109   -0.790160    0.704392  0.002683 -0.010570 -0.091689   5.291804  -2.303361   1.997900\n",
      "94   0.96350 -0.739661 -0.668873  0.074238 -0.012307 -0.096850 -0.995223  0.672867 -0.737041  0.063404   1.832714  -2.590063   3.166389    2.235805   -2.229535    2.673241 -0.030992 -0.000244 -0.059555   1.395912  -1.042306   7.658953\n",
      "95   0.97375 -0.739661 -0.668873  0.074238 -0.012307 -0.096850 -0.995223  0.672867 -0.737041  0.063404   3.699481  -3.181467  -0.520258    3.406301   -2.990792    0.119412 -0.024092  0.007624 -0.044827  -2.139913   1.750135  -2.304504\n",
      "96   0.98400 -0.740095 -0.668352  0.074591 -0.012323 -0.097420 -0.995167  0.672389 -0.737438  0.063864   2.891650   3.913228  -0.552834    2.994736    2.530326   -0.418180  0.013973 -0.015433 -0.084305   3.495106  -0.468016  -1.504315\n",
      "97   0.99425 -0.740095 -0.668352  0.074591 -0.012323 -0.097420 -0.995167  0.672389 -0.737438  0.063864   2.688309   0.949042   2.490346    2.749688    1.265779    1.907757 -0.028440  0.002351 -0.076677   2.179093  -0.259340  -0.041636\n",
      "98   1.00450 -0.740999 -0.667350  0.074599 -0.012223 -0.097668 -0.995144  0.671395 -0.738312  0.064215   3.851115  -5.778457  -3.121269    3.630495   -4.367469   -2.113936 -0.017061  0.007814 -0.086823  -6.061331   3.505966  -7.762902\n",
      "99   1.01475 -0.741916 -0.666298  0.074882 -0.012457 -0.097966 -0.995112  0.670377 -0.739222  0.064383   4.197219   4.939728   5.155388    4.083702    3.075460    3.699314 -0.000027 -0.003976 -0.046284   6.514240  -2.999038   3.225385\n",
      "100  1.02500 -0.741916 -0.666298  0.074882 -0.012457 -0.097966 -0.995112  0.670377 -0.739222  0.064383   1.163751  -4.814800  -2.078191    1.748628   -3.234350   -0.920935 -0.037960  0.011767 -0.086244  -4.709873   1.444213  -8.249167\n",
      "101  1.03525 -0.742554 -0.665547  0.075241 -0.012693 -0.098333 -0.995073  0.669666 -0.739850  0.064569   6.732381  -2.372828  -5.508772    5.734116   -2.545394   -4.589810  0.012762 -0.005115 -0.073011  -1.938786  -2.007356  -5.836618\n",
      "102  1.04550 -0.742554 -0.665547  0.075241 -0.012693 -0.098333 -0.995073  0.669666 -0.739850  0.064569  -0.243785  -0.026014   2.708108    0.953612   -0.530656    1.246307 -0.033809  0.011640 -0.100107  -1.753706  -1.087836   3.552481\n",
      "103  1.05575 -0.743536 -0.664422  0.075482 -0.013051 -0.098439 -0.995058  0.668569 -0.740846  0.064521   4.491504  -1.191295   3.426951    3.782851   -1.058967    2.990160 -0.007143 -0.000188 -0.069857   0.694138  -0.092509   2.384382\n",
      "104  1.06600 -0.744359 -0.663467  0.075773 -0.013301 -0.098717 -0.995027  0.667647 -0.741665  0.064656   0.577699   2.035375  -0.925146    1.219703    1.415566   -0.140895 -0.017506  0.005684 -0.081858  -0.796873   0.671273  -5.019902\n",
      "105  1.07625 -0.744359 -0.663467  0.075773 -0.013301 -0.098717 -0.995027  0.667647 -0.741665  0.064656   4.553934   1.447068  -3.037452    3.886075    1.440758   -2.457260 -0.022058  0.003041 -0.102126  -0.257880   0.306587  -2.532061\n",
      "106  1.08650 -0.745365 -0.662291  0.076168 -0.013735 -0.098974 -0.994995  0.666515 -0.742681  0.064676   4.088470  -0.086336  -0.127650    4.047930    0.219547   -0.594280 -0.032170  0.003486 -0.103817   1.199787   0.877080  -5.933024\n",
      "107  1.09675 -0.746460 -0.661031  0.076385 -0.013866 -0.099314 -0.994960  0.665285 -0.743757  0.064968   1.951991  -7.829875   3.589434    2.371816   -6.217544    2.751420  0.000422 -0.001394 -0.081768  -9.553410   5.700533   2.548928\n",
      "108  1.10700 -0.746460 -0.661031  0.076385 -0.013866 -0.099314 -0.994960  0.665285 -0.743757  0.064968   3.782265   7.405305  -1.507043    3.499747    4.676596   -0.654056 -0.031516  0.011040 -0.095101   9.082287  -4.399690  -0.716188\n",
      "109  1.11725 -0.747421 -0.659875  0.076984 -0.014368 -0.099796 -0.994904  0.664195 -0.744719  0.065108  -2.881183  -8.171644  -1.796439   -1.603058   -5.598092   -1.567615 -0.031936  0.015073 -0.097503 -12.641344   7.963314   1.146663\n",
      "110  1.12750 -0.747421 -0.659875  0.076984 -0.014368 -0.099796 -0.994904  0.664195 -0.744719  0.065108   2.794992   8.565338  -0.502467    1.914046    5.728348   -0.715820 -0.021582  0.008593 -0.093513   6.446908  -2.696416   0.190464\n",
      "111  1.13775 -0.748446 -0.658654  0.077485 -0.014727 -0.100300 -0.994848  0.663032 -0.745731  0.065369   5.878021  -7.397955   0.271858    5.084021   -4.768706    0.074023 -0.043466  0.017705 -0.101425  -6.159980   2.302768  -5.680268\n",
      "112  1.14800 -0.749565 -0.657330  0.077905 -0.015074 -0.100712 -0.994801  0.661759 -0.746843  0.065581   7.287127  -1.057863  -1.564737    6.845836   -1.801159   -1.236487 -0.016058  0.004616 -0.097101   0.381578  -2.949885  -4.477140\n",
      "113  1.15825 -0.749565 -0.657330  0.077905 -0.015074 -0.100712 -0.994801  0.661759 -0.746843  0.065581   4.409637   0.049016  -1.751343    4.897617   -0.321581   -1.648216 -0.024959  0.001638 -0.101040   3.233299  -2.078005  -2.095561\n",
      "114  1.16850 -0.750676 -0.656016  0.078286 -0.015381 -0.101109 -0.994756  0.660492 -0.747944  0.065809   5.404111  -3.248727   1.043915    5.302658   -2.662409    0.504671 -0.028782  0.005626 -0.090516  -1.820719   0.538016  -1.974059\n",
      "115  1.17875 -0.750676 -0.656016  0.078286 -0.015381 -0.101109 -0.994756  0.660492 -0.747944  0.065809   4.553399  -1.307839   2.609439    4.703479   -1.579164    2.187846  0.002980 -0.005163 -0.091643   0.012620  -0.345613  -1.566271\n",
      "116  1.18900 -0.751614 -0.654899  0.078637 -0.015718 -0.101401 -0.994721  0.659415 -0.748883  0.065921   3.975056   2.846166  -1.170383    4.120962    1.959756   -0.497717 -0.028938  0.013404 -0.124649   7.325946  -5.432881  -4.939904\n",
      "117  1.19925 -0.752873 -0.653409  0.078984 -0.016253 -0.101512 -0.994702  0.657965 -0.750168  0.065806   3.495734  -5.714470  -3.558992    3.620969   -4.177293   -2.945807  0.012247 -0.006498 -0.095556  -7.104427   2.495873   1.014118\n",
      "118  1.20950 -0.752873 -0.653409  0.078984 -0.016253 -0.101512 -0.994702  0.657965 -0.750168  0.065806   7.264702   3.046036  11.278547    6.534848    1.599175    8.429354  0.015266 -0.011392 -0.087409   9.083591  -7.122586   9.626727\n",
      "119  1.21975 -0.753752 -0.652386  0.079056 -0.016592 -0.101368 -0.994711  0.656949 -0.751077  0.065582   1.632119  -7.258906  -7.853776    2.614154   -5.484598   -4.592202 -0.025632  0.010326 -0.126588  -5.688837   3.932116 -13.967986\n",
      "120  1.23000 -0.754990 -0.650923  0.079314 -0.017332 -0.101102 -0.994725  0.655508 -0.752382  0.065049   5.544592   4.524000  -0.082174    4.957614    2.519239   -0.985550  0.075015 -0.035142 -0.100978   0.937044   0.056204   4.797114\n",
      "121  1.24025 -0.754990 -0.650923  0.079314 -0.017332 -0.101102 -0.994725  0.655508 -0.752382  0.065049   5.663876 -10.092674   6.174539    5.522409   -7.566459    4.740346 -0.014110  0.008217 -0.105840  -9.110633   6.142130   5.510954\n",
      "122  1.25050 -0.755939 -0.649817  0.079336 -0.017885 -0.100645 -0.994762  0.654397 -0.753398  0.064459  -3.513628   3.582482  -7.312614   -1.703675    1.349306   -4.898359  0.019289  0.004254 -0.086124   4.417419  -2.880317  -0.110348\n",
      "123  1.26075 -0.755939 -0.649817  0.079336 -0.017885 -0.100645 -0.994762  0.654397 -0.753398  0.064459  11.056599  -0.554959   4.857967    8.500666   -0.173527    2.903737  0.058908 -0.016222 -0.109389   7.085155  -4.437214  -1.617910\n",
      "124  1.27100 -0.757044 -0.648580  0.078922 -0.018415 -0.099564 -0.994861  0.653105 -0.754606  0.063431   0.669792 -11.456667   1.105414    2.238346   -9.196610    1.465625 -0.002102 -0.001957 -0.105616  -6.656054   0.407835  -0.710662\n",
      "125  1.28125 -0.758094 -0.647342  0.079003 -0.019333 -0.098783 -0.994921  0.651859 -0.755771  0.062371   4.349179  -5.301232   9.752356    3.926371   -6.081492    8.092492  0.067180 -0.030445 -0.094597   2.402764  -4.452063  10.248431\n",
      "126  1.29150 -0.758094 -0.647342  0.079003 -0.019333 -0.098783 -0.994921  0.651859 -0.755771  0.062371   8.376095  -8.002850  -1.312203    7.484798   -7.617995    0.571594  0.094043 -0.050868 -0.110573  -0.701412   1.428782  -5.392051\n",
      "127  1.30175 -0.759232 -0.646125  0.078034 -0.019477 -0.097290 -0.995065  0.650528 -0.757005  0.061281   9.333506   3.392160   0.362080    8.963203    1.186783    0.404046  0.026954 -0.015700 -0.114854   9.221405  -7.503435  -4.058910\n",
      "128  1.31200 -0.759232 -0.646125  0.078034 -0.019477 -0.097290 -0.995065  0.650528 -0.757005  0.061281   5.136080  -4.533408  -1.295964    5.902668   -3.387631   -0.955445  0.051813 -0.030854 -0.138891   5.398579  -3.516792  -1.980791\n",
      "129  1.32225 -0.760470 -0.644715  0.077643 -0.020111 -0.096127 -0.995166  0.649062 -0.758355  0.060135   6.738909  -0.368581   4.010620    6.571407   -0.973309    3.015898  0.098134 -0.056568 -0.095174   2.740419  -0.629821   2.078828\n",
      "130  1.33250 -0.761660 -0.643436  0.076574 -0.020160 -0.094586 -0.995313  0.647663 -0.759634  0.059071  -0.273604   3.716590  -2.791136    1.097478    2.777185   -1.627965  0.043556 -0.033921 -0.141662   3.886622  -0.665639   3.955609\n",
      "131  1.34275 -0.761660 -0.643436  0.076574 -0.020160 -0.094586 -0.995313  0.647663 -0.759634  0.059071   2.419942  -2.113270   6.477042    2.155047   -1.133693    4.853578  0.084616 -0.045935 -0.089190  -1.261602   1.901392   7.358347\n",
      "132  1.35300 -0.762899 -0.642100  0.075457 -0.019934 -0.093296 -0.995439  0.646211 -0.760923  0.058376  -0.097252   6.803660  -3.607862    0.353893    5.213777   -1.913003  0.076208 -0.040388 -0.101532   2.768580  -2.222706  -2.806666\n",
      "133  1.36325 -0.764160 -0.640732  0.074312 -0.019509 -0.092196 -0.995550  0.644732 -0.762209  0.057952   3.064574   0.655349   5.553314    2.521614    1.568420    4.057782  0.054107 -0.031952 -0.109896   2.566156  -1.286522   7.823774\n",
      "134  1.37350 -0.764160 -0.640732  0.074312 -0.019509 -0.092196 -0.995550  0.644732 -0.762209  0.057952   0.964116  -3.269140  -3.461072    1.276089   -2.300158   -1.955016  0.087431 -0.048321 -0.087320  -6.866019   5.254708  -3.670673\n",
      "135  1.38375 -0.765181 -0.639658  0.073038 -0.019047 -0.090905 -0.995677  0.643533 -0.763265  0.057376  -0.995453  -0.371254  -1.746684   -0.540455   -0.757621   -1.788414  0.061953 -0.035324 -0.101552  -2.378180  -0.062320  -0.846237\n",
      "136  1.39400 -0.765181 -0.639658  0.073038 -0.019047 -0.090905 -0.995677  0.643533 -0.763265  0.057376   2.966689   2.419877   2.750612    2.264194    1.783412    1.841427  0.074799 -0.042498 -0.114110   1.475066  -0.329320  -1.157930\n",
      "137  1.40425 -0.766402 -0.638354  0.071642 -0.018492 -0.089558 -0.995810  0.642096 -0.764515  0.056833   1.676040   3.399315  -1.540053    1.793850    3.075643   -0.862730  0.080904 -0.039262 -0.071592   3.600264  -1.635426  -2.748069\n",
      "138  1.41450 -0.767321 -0.637413  0.070160 -0.018057 -0.087889 -0.995967  0.641009 -0.765493  0.055929   4.961155  -3.559792  -4.197978    4.326731   -2.230689   -3.529915  0.052377 -0.025303 -0.087504  -6.327732   1.074921   1.020727\n",
      "139  1.42475 -0.767321 -0.637413  0.070160 -0.018057 -0.087889 -0.995967  0.641009 -0.765493  0.055929   3.997210   2.656650   6.478939    4.063215    1.677697    4.474127  0.087352 -0.046883 -0.097285   9.135104  -5.627837   5.380313\n",
      "140  1.43500 -0.768375 -0.636304  0.068680 -0.017524 -0.086354 -0.996110  0.639760 -0.766590  0.055201   4.145343   6.893950   0.277138    4.128892    5.849114    1.117811  0.093105 -0.053985 -0.079277   8.613182  -5.367133  -0.914980\n",
      "141  1.44525 -0.768375 -0.636304  0.068680 -0.017524 -0.086354 -0.996110  0.639760 -0.766590  0.055201   2.369092  -3.018176  -3.083123    2.721587   -1.242024   -2.241660  0.064060 -0.037044 -0.082659  -0.020382   3.026026  -6.452363\n",
      "142  1.45550 -0.769219 -0.635441  0.067212 -0.017056 -0.084730 -0.996258  0.638758 -0.767487  0.054338  -0.282824   3.821820   0.930634    0.318971    2.807512    0.295211  0.099769 -0.053809 -0.092738   0.193229   0.120835   5.946990\n",
      "143  1.46575 -0.770157 -0.634459  0.065733 -0.016615 -0.083063 -0.996406  0.637638 -0.768481  0.053430  -2.015142   1.764670  -0.521795   -1.547610    1.973555   -0.358146  0.077692 -0.034416 -0.073648  -4.266424   3.644868   2.809474\n",
      "144  1.47600 -0.770157 -0.634459  0.065733 -0.016615 -0.083063 -0.996406  0.637638 -0.768481  0.053430   2.638111   3.843723   0.581303    1.799695    3.469121    0.393128  0.101365 -0.045715 -0.075176   5.783822  -1.387600   3.201407\n",
      "145  1.48625 -0.770985 -0.633590  0.064384 -0.016342 -0.081382 -0.996549  0.636644 -0.769377  0.052390   2.129413   2.235773   6.218920    2.063369    2.482818    5.051992  0.058727 -0.030482 -0.092544   4.196445  -1.370471   8.961252\n",
      "146  1.49650 -0.771830 -0.632696  0.063046 -0.015911 -0.079906 -0.996675  0.635630 -0.770267  0.051607   1.658071  -0.037759  -7.741832    1.739254    0.467122   -5.179180  0.056032 -0.018267 -0.083870  -6.423313   5.106397 -14.072485\n",
      "147  1.50675 -0.771830 -0.632696  0.063046 -0.015911 -0.079906 -0.996675  0.635630 -0.770267  0.051607   3.447822  -6.211007  -3.234150    3.105589   -4.873352   -3.623747  0.088367 -0.035732 -0.072684  -8.032800   5.162989 -10.717186\n",
      "148  1.51700 -0.772761 -0.631666  0.061959 -0.015909 -0.078311 -0.996802  0.634498 -0.771275  0.050466   1.385512   1.655332   8.620486    1.730050    0.347612    6.167919  0.068206 -0.032184 -0.080330   5.734704  -1.075163  11.371309\n",
      "149  1.52725 -0.772761 -0.631666  0.061959 -0.015909 -0.078311 -0.996802  0.634498 -0.771275  0.050466   8.267664   4.543197   0.574756    6.958154    3.702805    1.695088  0.022616  0.007713 -0.089017   9.330852  -6.967935   4.195065\n",
      "150  1.53750 -0.773534 -0.630807  0.061049 -0.015829 -0.077068 -0.996900  0.633556 -0.772103  0.049630   3.667384 -10.386424   4.624856    4.326538   -7.564297    4.038012  0.063268 -0.023611 -0.097409  -7.649074   3.229645   1.891876\n",
      "151  1.54775 -0.774518 -0.629706  0.059928 -0.015731 -0.075536 -0.997019  0.632356 -0.773152  0.048598   2.704352  -7.736769  -0.315403    3.029282   -7.702222    0.556603  0.066704 -0.039152 -0.059722  -4.495643   0.449412   5.925894\n",
      "152  1.55800 -0.774518 -0.629706  0.059928 -0.015731 -0.075536 -0.997019  0.632356 -0.773152  0.048598   4.416223   6.312998  -4.515742    4.138413    3.505695   -3.499732  0.081790 -0.033086 -0.079206  14.797402  -7.924433  -2.638710\n",
      "153  1.56825 -0.775358 -0.628755  0.059056 -0.015917 -0.074028 -0.997129  0.631321 -0.774072  0.047390   0.048596   0.852770   9.302157    0.867802    1.384161    6.737889  0.041549 -0.018292 -0.109751  -5.330526   4.418629   9.983940\n",
      "154  1.57850 -0.775358 -0.628755  0.059056 -0.015917 -0.074028 -0.997129  0.631321 -0.774072  0.047390   6.785982  -4.908619  -7.756619    5.600547   -3.648151   -4.853313  0.041429 -0.017005 -0.082200  -1.587135   3.783320 -20.923968\n",
      "155  1.58875 -0.776254 -0.627727  0.058214 -0.015843 -0.072888 -0.997214  0.630221 -0.775014  0.046635   0.010439  -4.649951  -3.780373    1.130159   -4.449286   -3.995287  0.091518 -0.024877 -0.091121  -8.611807   3.787625  -3.654542\n",
      "156  1.59900 -0.777344 -0.626436  0.057573 -0.016424 -0.071279 -0.997321  0.628861 -0.776207  0.045119   3.405707   7.178556   6.228461    2.949906    4.849454    4.180605  0.040164 -0.019482 -0.090167  12.169207  -7.469684   6.140701\n",
      "157  1.60925 -0.777344 -0.626436  0.057573 -0.016424 -0.071279 -0.997321  0.628861 -0.776207  0.045119   1.059830  -5.651150   0.799796    1.438419   -3.547838    1.476985  0.052370 -0.014454 -0.109457  -6.759916   6.587566  10.001080\n",
      "158  1.61950 -0.778242 -0.625374  0.056984 -0.016662 -0.070147 -0.997397  0.627743 -0.777166  0.044172   3.104918  -8.482940   6.238306    2.771112   -7.494420    5.284595  0.076815 -0.032241 -0.112993  -7.339770   1.705952   2.215762\n",
      "159  1.62975 -0.779342 -0.624071  0.056221 -0.016802 -0.068878 -0.997484  0.626373 -0.778326  0.043194   1.437987   6.989679  -8.594040    1.705017    4.088458   -5.814096  0.045451 -0.026786 -0.104134   7.440225  -2.810529  -7.167264\n",
      "160  1.64000 -0.779342 -0.624071  0.056221 -0.016802 -0.068878 -0.997484  0.626373 -0.778326  0.043194   1.208768   2.312483  -1.189019    1.308169    2.668218   -2.115440  0.052801 -0.014524 -0.104855  -1.706216   1.939033   1.994268\n",
      "161  1.65025 -0.780327 -0.622907  0.055465 -0.017025 -0.067499 -0.997574  0.625140 -0.779379  0.042067  -0.685717  -4.036284   3.282900   -0.286334   -2.693346    2.201592  0.042103 -0.023692 -0.127477  -8.827270   7.026782   6.687272\n",
      "162  1.66050 -0.780327 -0.622907  0.055465 -0.017025 -0.067499 -0.997574  0.625140 -0.779379  0.042067   6.102400   2.645572  -6.835028    4.822712    1.576166   -5.024958  0.034323 -0.019145 -0.082674   4.165069  -2.711821  -6.131330\n",
      "163  1.67075 -0.781375 -0.621674  0.054542 -0.016675 -0.066569 -0.997642  0.623839 -0.780442  0.041649   9.582165  -0.226456  -0.273026    8.628828    0.134616   -1.224856  0.059613 -0.021671 -0.104631  12.082896  -5.995436  -6.795534\n",
      "164  1.68100 -0.782567 -0.620267  0.053466 -0.016375 -0.065343 -0.997729  0.622351 -0.781665  0.040978   5.018437   1.191815  -1.616212    5.741612    0.980054   -1.537822  0.035450 -0.019778 -0.082929   0.635169  -3.199254  -2.637401\n",
      "165  1.69125 -0.782567 -0.620267  0.053466 -0.016375 -0.065343 -0.997729  0.622351 -0.781665  0.040978   1.914658   4.265468  -2.970046    2.681212    3.607387   -2.683166  0.068320 -0.049299 -0.096346   4.006843  -1.156510  -4.714544\n",
      "166  1.70150 -0.783569 -0.619072  0.052625 -0.015918 -0.064669 -0.997780  0.621101 -0.782667  0.040818  -0.568912  -3.868506   3.828655    0.082100   -2.371056    2.524312  0.097574 -0.072005 -0.092235 -11.087620   9.195174  -1.585585\n",
      "167  1.71175 -0.783569 -0.619072  0.052625 -0.015918 -0.064669 -0.997780  0.621101 -0.782667  0.040818  -3.659906   1.705525  -3.485112   -2.910368    0.888970   -2.281402  0.054018 -0.039953 -0.090051  -6.839550   7.006107   2.196721\n",
      "168  1.72200 -0.784636 -0.617836  0.051226 -0.014996 -0.063689 -0.997857  0.619775 -0.783723  0.040707  -2.199807  -2.834829   3.473712   -2.342135   -2.088938    2.320940  0.075743 -0.041517 -0.095564  -4.800128   8.833041   6.959709\n",
      "169  1.73225 -0.785764 -0.616518  0.049806 -0.013963 -0.062822 -0.997927  0.618369 -0.784830  0.040755  -1.919771   2.867306  -6.946850   -2.004372    1.874551   -5.090475  0.033904 -0.012641 -0.105670  -1.652609   3.500233  -5.471571\n",
      "170  1.74250 -0.785764 -0.616518  0.049806 -0.013963 -0.062822 -0.997927  0.618369 -0.784830  0.040755   3.993354  -2.453787   2.984765    2.791986   -1.586804    1.367263  0.020985 -0.016432 -0.102584   0.189311  -2.143900   2.401892\n",
      "171  1.75275 -0.786838 -0.615242  0.048622 -0.013059 -0.062168 -0.997980  0.617022 -0.785884  0.040881   4.319754  -8.933999  -5.756443    4.013737   -7.462327   -4.329537  0.055768 -0.030101 -0.096439  -9.839819   3.003364  -7.227267\n",
      "172  1.76300 -0.786838 -0.615242  0.048622 -0.013059 -0.062168 -0.997980  0.617022 -0.785884  0.040881   5.067491  -5.443832   1.773016    4.856420   -5.848145    0.550651  0.041515 -0.027611 -0.116061  -8.059244   2.086611  -4.889524\n",
      "173  1.77325 -0.787943 -0.613941  0.047137 -0.012002 -0.061224 -0.998052  0.615631 -0.786974  0.040872   3.465217  -4.587904   1.830106    3.743880   -4.840335    1.573826  0.083641 -0.055944 -0.102616  -5.728370   3.694555   3.948592\n",
      "174  1.78350 -0.789100 -0.612581  0.045461 -0.010554 -0.060478 -0.998114  0.614175 -0.788091  0.041258   3.246901   4.991161  -7.657899    3.346448    3.021875   -5.808748  0.034181 -0.024695 -0.114834   5.839484  -3.730031  -6.338876\n",
      "175  1.79375 -0.789100 -0.612581  0.045461 -0.010554 -0.060478 -0.998114  0.614175 -0.788091  0.041258   4.105100   1.262013   0.868502    3.953139    1.614520   -0.468977  0.078046 -0.059255 -0.088595   3.906781  -2.336674   0.650784\n",
      "176  1.80400 -0.790245 -0.611226  0.043762 -0.009129 -0.059664 -0.998177  0.612723 -0.789204  0.041570   5.092452  -1.075375  -0.344224    4.864243   -0.536579   -0.369213  0.046365 -0.039748 -0.105916   3.534018  -2.376581   2.459467\n",
      "177  1.81425 -0.791462 -0.609747  0.042384 -0.007988 -0.059019 -0.998225  0.611166 -0.790396  0.041841   9.304402   2.150494  -4.987342    8.415021    1.612263   -4.062313  0.068630 -0.052489 -0.118295   6.244219  -6.030010 -15.728286\n",
      "178  1.82450 -0.791462 -0.609747  0.042384 -0.007988 -0.059019 -0.998225  0.611166 -0.790396  0.041841  -3.211969  -3.986209   0.733725   -0.883038   -2.864813   -0.226940  0.063764 -0.048400 -0.116444 -12.572309   5.437196   5.453935\n",
      "179  1.83475 -0.792731 -0.608216  0.040636 -0.006589 -0.058110 -0.998288  0.609536 -0.791642  0.042058   5.933506   7.260274   4.372281    4.568126    5.232180    3.451039  0.072618 -0.050341 -0.130277  12.215822  -5.143703   4.242078\n",
      "180  1.84500 -0.792731 -0.608216  0.040636 -0.006589 -0.058110 -0.998288  0.609536 -0.791642  0.042058   0.461972  -9.287420  -1.606428    1.284451   -6.379088   -0.593398  0.049987 -0.024980 -0.132078 -17.176167  12.594018  -9.705578\n",
      "181  1.85525 -0.794113 -0.606529  0.038813 -0.005345 -0.056889 -0.998366  0.607746 -0.793023  0.041934   2.657929   7.237940   2.043962    2.382816    4.510396    1.515689  0.061717 -0.030550 -0.118299  10.206771  -4.809264   0.079637\n",
      "182  1.86550 -0.795447 -0.604872  0.037337 -0.004527 -0.055677 -0.998439  0.606007 -0.794374  0.041550   7.285795 -13.320169  -1.997217    6.303709   -9.748637   -1.293569  0.075566 -0.036279 -0.153859  -9.609132   4.668656  -4.658717\n",
      "183  1.87575 -0.795447 -0.604872  0.037337 -0.004527 -0.055677 -0.998439  0.606007 -0.794374  0.041550  -1.157630   4.411962  -1.079994    0.336905    1.575539   -1.122774  0.090878 -0.053567 -0.112916   0.029742  -0.056471  -2.642417\n",
      "184  1.88600 -0.796825 -0.603131  0.036087 -0.004246 -0.054135 -0.998525  0.604195 -0.795803  0.040575   5.675631   0.435586   1.329449    4.606263    0.663923    0.838259  0.089469 -0.040836 -0.164702   5.930180  -0.113724   3.640311\n",
      "185  1.89625 -0.796825 -0.603131  0.036087 -0.004246 -0.054135 -0.998525  0.604195 -0.795803  0.040575   0.333078  -2.565277   2.263333    1.189013   -1.918456    1.977885  0.059066 -0.017660 -0.128799  -6.041035   2.631443   0.030796\n",
      "186  1.90650 -0.798344 -0.601188  0.034931 -0.004333 -0.052269 -0.998624  0.602187 -0.797396  0.039124   7.183118   1.531943   2.218843    5.982475    0.840815    2.170578  0.104806 -0.038634 -0.154738   6.742457  -5.874061   6.376778\n",
      "187  1.91675 -0.799822 -0.599283  0.033846 -0.004580 -0.050292 -0.998724  0.600220 -0.798956  0.037480   2.530771  -1.952245  -0.315887    3.222160   -1.392785    0.182162  0.057229 -0.017694 -0.156957   0.533497  -1.696445   0.782956\n",
      "188  1.92700 -0.799822 -0.599283  0.033846 -0.004580 -0.050292 -0.998724  0.600220 -0.798956  0.037480   7.353247   1.154975   1.401803    6.525774    0.644649    1.157504  0.102381 -0.041201 -0.154609   5.439910  -5.091513  -0.216074\n",
      "189  1.93725 -0.801319 -0.597318  0.033149 -0.005272 -0.048357 -0.998816  0.598214 -0.800545  0.035600   1.378732   6.101633  -1.937280    2.409705    5.008578   -1.317383  0.066246 -0.020022 -0.176540   5.674406   0.084368  -3.427829\n",
      "190  1.94750 -0.802925 -0.595185  0.032636 -0.006201 -0.046408 -0.998903  0.596047 -0.802247  0.033571   0.272360   3.328220  -0.425605    0.700478    3.664802   -0.604232  0.059114 -0.012820 -0.154179  -4.353842   3.258247   4.145113\n",
      "191  1.95775 -0.802925 -0.595185  0.032636 -0.006201 -0.046408 -0.998903  0.596047 -0.802247  0.033571   5.963883  -0.627613  -3.382112    4.909603    0.232174   -2.825692  0.072727 -0.017403 -0.178625  -0.254232   2.506646  -5.944883\n",
      "192  1.96800 -0.804545 -0.593014  0.032266 -0.007267 -0.044497 -0.998983  0.593847 -0.803962  0.031490   6.013439   7.705295  -1.136661    5.792336    6.208400   -1.474981  0.045147 -0.008396 -0.157112   7.007642  -5.963675  -6.957731\n",
      "193  1.97825 -0.804545 -0.593014  0.032266 -0.007267 -0.044497 -0.998983  0.593847 -0.803962  0.031490   8.528804  -2.811703   0.442468    7.980679   -1.004941    0.058396  0.065509 -0.016989 -0.184832  -4.887392   4.930366  -1.866416\n",
      "194  1.98850 -0.806138 -0.590867  0.031912 -0.008242 -0.042712 -0.999053  0.591670 -0.805638  0.029562  -1.165962   2.952334  -2.643325    0.666145    2.159676   -2.102160  0.040802 -0.005555 -0.163943   0.515340  -1.361768   0.288677\n",
      "195  1.99875 -0.807756 -0.588661  0.031749 -0.009413 -0.040970 -0.999116  0.589441 -0.807341  0.027552   3.784606  -7.443021   0.590129    3.159966   -5.519563    0.050853  0.057282 -0.007180 -0.192535  -9.198660   4.840453   1.074367\n",
      "196  2.00900 -0.807756 -0.588661  0.031749 -0.009413 -0.040970 -0.999116  0.589441 -0.807341  0.027552   3.940017   5.978105  -3.669788    3.783770    3.675078   -2.924530  0.048515 -0.011770 -0.170798  11.081914  -8.386156  -5.862943\n",
      "197  2.01925 -0.809436 -0.586356  0.031619 -0.010491 -0.039396 -0.999169  0.587115 -0.809095  0.025737   5.669388 -10.166806   1.242089    5.291691   -7.394223    0.407499  0.052148 -0.011880 -0.187831  -9.713167   5.672253  -1.502865\n",
      "198  2.02950 -0.809436 -0.586356  0.031619 -0.010491 -0.039396 -0.999169  0.587115 -0.809095  0.025737   2.264416   8.859484  -4.406924    2.870791    5.603803   -3.442576  0.073140 -0.028941 -0.172032   9.455831  -4.810768  -1.999118\n",
      "199  2.03975 -0.811121 -0.584026  0.031562 -0.011635 -0.037840 -0.999216  0.584762 -0.810853  0.023898   3.087998  -5.652066   2.789213    3.044491   -3.397472    1.540961  0.022391  0.007934 -0.181064  -8.287092   6.795294   0.461418\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is your Scheme\n",
    "# data = pd.DataFrame(columns=[\n",
    "#     'imuId',\n",
    "#     'vehicleAggression',\n",
    "#     'time',\n",
    "#     'pos',\n",
    "#     'dirX',\n",
    "#     'dirY',\n",
    "#     'dirZ',\n",
    "#     'angVel',\n",
    "#     'angAccel',\n",
    "#     'mass',\n",
    "#     'accRaw',\n",
    "#     'accSmooth'\n",
    "# ])\n",
    "\n",
    "# Function to split the data into chunks\n",
    "def split_into_chunks(data, chunk_size):\n",
    "    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "# Group the data by 'imuId' and sort within each group by 'time'\n",
    "grouped_data = data.groupby('imuId').apply(lambda x: x.sort_values('time')).reset_index(drop=True)\n",
    "\n",
    "# Set the desired chunk size (number of records per chunk)\n",
    "chunk_size = 200\n",
    "\n",
    "# Split the data into chunks and assign the 'vehicleAggression' value as the label\n",
    "training_data = []\n",
    "for imu_id, group in grouped_data.groupby('imuId'):\n",
    "    chunks = split_into_chunks(group, chunk_size)\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) >= chunk_size:\n",
    "            label = chunk['vehicleAggression'].iloc[0]\n",
    "            first_timestamp = chunk['time'].iloc[0]\n",
    "            adjusted_time = chunk['time'] - first_timestamp\n",
    "            \n",
    "            # Separate list columns into individual columns\n",
    "            # pos_df = pd.DataFrame(chunk['pos'].tolist(), columns=['posX', 'posY', 'posZ'], index=chunk.index)\n",
    "            dir_x_df = pd.DataFrame(chunk['dirX'].tolist(), columns=['dirXX', 'dirXY', 'dirXZ'], index=chunk.index)\n",
    "            dir_y_df = pd.DataFrame(chunk['dirY'].tolist(), columns=['dirYX', 'dirYY', 'dirYZ'], index=chunk.index)\n",
    "            dir_z_df = pd.DataFrame(chunk['dirZ'].tolist(), columns=['dirZX', 'dirZY', 'dirZZ'], index=chunk.index)\n",
    "            acc_raw_df = pd.DataFrame(chunk['accRaw'].tolist(), columns=['accRawX', 'accRawY', 'accRawZ'], index=chunk.index)\n",
    "            acc_smooth_df = pd.DataFrame(chunk['accSmooth'].tolist(), columns=['accSmoothX', 'accSmoothY', 'accSmoothZ'], index=chunk.index)\n",
    "            ang_vel_df = pd.DataFrame(chunk['angVel'].tolist(), columns=['angVelX', 'angVelY', 'angVelZ'], index=chunk.index)\n",
    "            ang_accel_df = pd.DataFrame(chunk['angAccel'].tolist(), columns=['angAccelX', 'angAccelY', 'angAccelZ'], index=chunk.index)\n",
    "            \n",
    "            expanded_chunk = pd.concat(\n",
    "                [\n",
    "                    chunk,\n",
    "                    # pos_df,\n",
    "                    dir_x_df,\n",
    "                    dir_y_df,\n",
    "                    dir_z_df,\n",
    "                    acc_raw_df,\n",
    "                    acc_smooth_df,\n",
    "                    ang_vel_df,\n",
    "                    ang_accel_df\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            updated_chunk = (\n",
    "                expanded_chunk.assign(time=adjusted_time)\n",
    "                .drop(['imuId', 'mass', 'vehicleAggression', 'pos', 'dirX', 'dirY', 'dirZ', 'angVel', 'angAccel', 'accRaw', 'accSmooth'], axis=1)\n",
    "            )\n",
    "            \n",
    "            training_data.append({'data': updated_chunk, 'label': label})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "training_data_df = pd.DataFrame(training_data)\n",
    "\n",
    "# Example of a single training set\n",
    "# Set display options\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 100)  # Set the maximum column width to 100 characters\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "print(training_data_df.loc[0, 'data'])\n",
    "pd.reset_option(\"all\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating the training data to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1267, 200, 22)\n",
      "X_test shape: (317, 200, 22)\n",
      "y_train shape: (1267,)\n",
      "y_test shape: (317,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the data and labels from the training_data_df\n",
    "X = np.stack(training_data_df['data'].apply(lambda x: x.to_numpy()).to_numpy())\n",
    "y = training_data_df['label'].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn with PyTorch\n",
    "\n",
    "- Create a TensorDataset\n",
    "- Create a DataLoader, which shuffles the data\n",
    "- Create a simple neural net (torch.nn.Sequential) which uses CUDA while training\n",
    "- Train the neural net with the data provided\n",
    "- Evaluate the net with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# # Pad sequences to the same length\n",
    "# X_train_padded = pad_sequences(X_train, dtype='float32', padding='post')\n",
    "# y_train_padded = pad_sequences(y_train, dtype='float32', padding='post')\n",
    "# X_test_padded = pad_sequences(X_test, dtype='float32', padding='post')\n",
    "# y_test_padded = pad_sequences(y_test, dtype='float32', padding='post')\n",
    "\n",
    "# Create tensors from the padded data\n",
    "X_train_tensor = torch.tensor(X_train).permute(0, 2, 1)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "X_test_tensor = torch.tensor(X_test).permute(0, 2, 1)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the CNN architecture\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels, num_filters, kernel_size, pool_size, hidden_units, dropout_rate, device):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, num_filters, kernel_size),\n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(pool_size)\n",
    "        ).to(device)\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(num_filters, num_filters * 2, kernel_size),\n",
    "            nn.BatchNorm1d(num_filters * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(pool_size)\n",
    "        ).to(device)\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(num_filters * 2, num_filters * 4, kernel_size),\n",
    "            nn.BatchNorm1d(num_filters * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(pool_size)\n",
    "        ).to(device)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        conv1_out_size = (chunk_size - kernel_size + 1) // pool_size\n",
    "        conv2_out_size = (conv1_out_size - kernel_size + 1) // pool_size\n",
    "        conv3_out_size = (conv2_out_size - kernel_size + 1) // pool_size\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(num_filters * 4 * conv3_out_size, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ).to(device)\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(hidden_units, hidden_units // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ).to(device)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_units // 2, 1).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_channels = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_and_eval_net(trial):\n",
    "    # Suggest hyperparameters using the trial object\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 256, 512)\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 3, 5)\n",
    "    pool_size = trial.suggest_int(\"pool_size\", 2, 4)\n",
    "    hidden_units = trial.suggest_int(\"hidden_units\", 24, 92)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    \n",
    "    # Value: 0.029481003992259502\n",
    "    # Params: {'num_filters': 454, 'kernel_size': 3, 'pool_size': 4, 'hidden_units': 53, 'dropout_rate': 0.15038840244397983, 'learning_rate': 5.2460887662900957e-05}\n",
    "    # num_filters = 454\n",
    "    # kernel_size = 3\n",
    "    # pool_size = 4\n",
    "    # hidden_units = 53\n",
    "    # dropout_rate = 0.15038840244397983\n",
    "    # learning_rate = 5.2460887662900957e-05]\n",
    "    \n",
    "    # Value: 0.028036039788275957\n",
    "    # Params: {'num_filters': 437, 'kernel_size': 4, 'pool_size': 4, 'hidden_units': 73, 'dropout_rate': 0.1657660256105821, 'learning_rate': 3.964884325777312e-05}\n",
    "    # num_filters = 437\n",
    "    # kernel_size = 4\n",
    "    # pool_size = 4\n",
    "    # hidden_units = 73\n",
    "    # dropout_rate = 0.1657660256105821\n",
    "    # learning_rate = 3.964884325777312e-05\n",
    "    \n",
    "    # Create the model\n",
    "    net = CNNRegressor(input_channels, num_filters, kernel_size, pool_size, hidden_units, dropout_rate, device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    num_epochs = 50\n",
    "    \n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs} Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        total_difference = 0.0\n",
    "        fake_difference = 0.0\n",
    "        num_samples = 0\n",
    "        test_loss = 0.0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Calculate the absolute difference between the predicted and real labels\n",
    "            difference = torch.abs(outputs.squeeze() - labels)\n",
    "\n",
    "            # Update the total difference and the number of samples\n",
    "            total_difference += difference.sum().item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # random outputs\n",
    "            fake_outputs = torch.tensor([random.uniform(0.2, 0.6) for _ in range(len(outputs))], device=device)\n",
    "            fake_diff = torch.abs(fake_outputs - labels)\n",
    "            fake_difference += fake_diff.sum().item()\n",
    "\n",
    "        print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "        # Calculate the average absolute difference\n",
    "        average_difference = total_difference / num_samples\n",
    "        average_fake_difference = fake_difference / num_samples\n",
    "        print(f\"Average Absolute Difference: {average_difference}\")\n",
    "        print(f\"Average Absolute Fake Difference: {average_fake_difference}\")\n",
    "    \n",
    "    return test_loss/len(test_loader)\n",
    "\n",
    "# train_and_eval_net(None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization via Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:29:11,708]\u001b[0m A new study created in memory with name: no-name-d82a1031-17fe-4aba-9d21-210045bf08bb\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:30:04,833]\u001b[0m Trial 0 finished with value: 0.10289446761210759 and parameters: {'num_filters': 407, 'kernel_size': 3, 'pool_size': 2, 'hidden_units': 29, 'dropout_rate': 0.3719176929230378, 'learning_rate': 0.0008220190951594898}. Best is trial 0 with value: 0.10289446761210759.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.10289446761210759\n",
      "Average Absolute Difference: 0.2682291746891635\n",
      "Average Absolute Fake Difference: 0.23194245058655363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:31:27,400]\u001b[0m Trial 1 finished with value: 0.031783935924371086 and parameters: {'num_filters': 331, 'kernel_size': 4, 'pool_size': 2, 'hidden_units': 57, 'dropout_rate': 0.13682306138658817, 'learning_rate': 2.343593459453175e-05}. Best is trial 1 with value: 0.031783935924371086.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.031783935924371086\n",
      "Average Absolute Difference: 0.14330925926425103\n",
      "Average Absolute Fake Difference: 0.2210026148365875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:33:24,733]\u001b[0m Trial 2 finished with value: 0.03317089999715487 and parameters: {'num_filters': 485, 'kernel_size': 5, 'pool_size': 2, 'hidden_units': 91, 'dropout_rate': 0.21832811639189167, 'learning_rate': 1.705668841037158e-05}. Best is trial 1 with value: 0.031783935924371086.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.03317089999715487\n",
      "Average Absolute Difference: 0.1484018801138604\n",
      "Average Absolute Fake Difference: 0.2261171762123469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:33:42,715]\u001b[0m Trial 3 finished with value: 0.04027862846851349 and parameters: {'num_filters': 268, 'kernel_size': 3, 'pool_size': 3, 'hidden_units': 86, 'dropout_rate': 0.17755914447901355, 'learning_rate': 5.0616206953520196e-05}. Best is trial 1 with value: 0.031783935924371086.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.04027862846851349\n",
      "Average Absolute Difference: 0.15956869636800386\n",
      "Average Absolute Fake Difference: 0.23076054726489334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:34:29,072]\u001b[0m Trial 4 finished with value: 0.040174401054779686 and parameters: {'num_filters': 293, 'kernel_size': 3, 'pool_size': 2, 'hidden_units': 48, 'dropout_rate': 0.2422479593225415, 'learning_rate': 0.0007194908932876344}. Best is trial 1 with value: 0.031783935924371086.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.040174401054779686\n",
      "Average Absolute Difference: 0.16241896114890883\n",
      "Average Absolute Fake Difference: 0.2286787394093414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:36:34,897]\u001b[0m Trial 5 finished with value: 0.08964213232199351 and parameters: {'num_filters': 475, 'kernel_size': 3, 'pool_size': 2, 'hidden_units': 58, 'dropout_rate': 0.42949097939401437, 'learning_rate': 0.00028860129552310223}. Best is trial 1 with value: 0.031783935924371086.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.08964213232199351\n",
      "Average Absolute Difference: 0.24918982809650408\n",
      "Average Absolute Fake Difference: 0.23681144233005655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:37:16,971]\u001b[0m Trial 6 finished with value: 0.04451762139797211 and parameters: {'num_filters': 423, 'kernel_size': 4, 'pool_size': 3, 'hidden_units': 47, 'dropout_rate': 0.4678835839288794, 'learning_rate': 7.325706255704478e-05}. Best is trial 1 with value: 0.031783935924371086.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.04451762139797211\n",
      "Average Absolute Difference: 0.17199270206294978\n",
      "Average Absolute Fake Difference: 0.23941023492662689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:39:00,388]\u001b[0m Trial 7 finished with value: 0.03547012433409691 and parameters: {'num_filters': 314, 'kernel_size': 4, 'pool_size': 2, 'hidden_units': 74, 'dropout_rate': 0.41631069321125147, 'learning_rate': 1.0489986108054175e-05}. Best is trial 1 with value: 0.031783935924371086.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.03547012433409691\n",
      "Average Absolute Difference: 0.15485340262813146\n",
      "Average Absolute Fake Difference: 0.22415540797477265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:40:16,859]\u001b[0m Trial 8 finished with value: 0.047294648985068 and parameters: {'num_filters': 472, 'kernel_size': 4, 'pool_size': 3, 'hidden_units': 53, 'dropout_rate': 0.3151563927514621, 'learning_rate': 1.8288234984518414e-05}. Best is trial 1 with value: 0.031783935924371086.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.047294648985068\n",
      "Average Absolute Difference: 0.17797255892106786\n",
      "Average Absolute Fake Difference: 0.22169329766595403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:40:43,651]\u001b[0m Trial 9 finished with value: 0.06189605345328649 and parameters: {'num_filters': 346, 'kernel_size': 3, 'pool_size': 3, 'hidden_units': 34, 'dropout_rate': 0.38966613128679073, 'learning_rate': 1.8784388035109973e-05}. Best is trial 1 with value: 0.031783935924371086.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.06189605345328649\n",
      "Average Absolute Difference: 0.2063645052985062\n",
      "Average Absolute Fake Difference: 0.22820987761584743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:41:05,505]\u001b[0m Trial 10 finished with value: 0.028469223529100418 and parameters: {'num_filters': 369, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 70, 'dropout_rate': 0.10084933148470171, 'learning_rate': 4.325122114456011e-05}. Best is trial 10 with value: 0.028469223529100418.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.028469223529100418\n",
      "Average Absolute Difference: 0.1353539803802779\n",
      "Average Absolute Fake Difference: 0.2215993562331335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:41:27,166]\u001b[0m Trial 11 finished with value: 0.028658822799722355 and parameters: {'num_filters': 360, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 69, 'dropout_rate': 0.10264323081169152, 'learning_rate': 4.024276019450269e-05}. Best is trial 10 with value: 0.028469223529100418.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.028658822799722355\n",
      "Average Absolute Difference: 0.1356714982715691\n",
      "Average Absolute Fake Difference: 0.23819285612377084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:41:48,811]\u001b[0m Trial 12 finished with value: 0.028858947878082592 and parameters: {'num_filters': 360, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 72, 'dropout_rate': 0.11153810108298234, 'learning_rate': 5.362206874806758e-05}. Best is trial 10 with value: 0.028469223529100418.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.028858947878082592\n",
      "Average Absolute Difference: 0.13755925596700483\n",
      "Average Absolute Fake Difference: 0.23497712649757554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:42:40,623]\u001b[0m Trial 13 finished with value: 0.031954143196344376 and parameters: {'num_filters': 385, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 70, 'dropout_rate': 0.1118777190246093, 'learning_rate': 0.00011340072897117905}. Best is trial 10 with value: 0.028469223529100418.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.031954143196344376\n",
      "Average Absolute Difference: 0.13947017561374025\n",
      "Average Absolute Fake Difference: 0.2271132905400514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:44:13,792]\u001b[0m Trial 14 finished with value: 0.03980593383312225 and parameters: {'num_filters': 428, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 79, 'dropout_rate': 0.16915678782978172, 'learning_rate': 3.390992372404934e-05}. Best is trial 10 with value: 0.028469223529100418.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.03980593383312225\n",
      "Average Absolute Difference: 0.15985609830742004\n",
      "Average Absolute Fake Difference: 0.22898986286921455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:45:26,393]\u001b[0m Trial 15 finished with value: 0.031221793964505196 and parameters: {'num_filters': 375, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 64, 'dropout_rate': 0.10224158128898073, 'learning_rate': 0.0001456608120918707}. Best is trial 10 with value: 0.028469223529100418.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.031221793964505196\n",
      "Average Absolute Difference: 0.14097715552296924\n",
      "Average Absolute Fake Difference: 0.23218961469006463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:45:54,725]\u001b[0m Trial 16 finished with value: 0.03418207665284475 and parameters: {'num_filters': 446, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 65, 'dropout_rate': 0.1720122088664625, 'learning_rate': 3.937972513796693e-05}. Best is trial 10 with value: 0.028469223529100418.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.03418207665284475\n",
      "Average Absolute Difference: 0.14775852600482736\n",
      "Average Absolute Fake Difference: 0.21988496268961333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:46:20,042]\u001b[0m Trial 17 finished with value: 0.03176876219610373 and parameters: {'num_filters': 397, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 80, 'dropout_rate': 0.24156398932131568, 'learning_rate': 8.611938425976827e-05}. Best is trial 10 with value: 0.028469223529100418.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.03176876219610373\n",
      "Average Absolute Difference: 0.14066249414197277\n",
      "Average Absolute Fake Difference: 0.228411713609184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:46:57,948]\u001b[0m Trial 18 finished with value: 0.048931521673997246 and parameters: {'num_filters': 510, 'kernel_size': 4, 'pool_size': 4, 'hidden_units': 63, 'dropout_rate': 0.2937397190619352, 'learning_rate': 3.1259054639587634e-05}. Best is trial 10 with value: 0.028469223529100418.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.048931521673997246\n",
      "Average Absolute Difference: 0.1795970724208122\n",
      "Average Absolute Fake Difference: 0.23902514304272385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 14:47:19,635]\u001b[0m Trial 19 finished with value: 0.03489603102207184 and parameters: {'num_filters': 308, 'kernel_size': 5, 'pool_size': 3, 'hidden_units': 40, 'dropout_rate': 0.1485637354037996, 'learning_rate': 5.788461171859879e-05}. Best is trial 10 with value: 0.028469223529100418.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.03489603102207184\n",
      "Average Absolute Difference: 0.14860170045485632\n",
      "Average Absolute Fake Difference: 0.23002482814367636\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(train_and_eval_net, n_trials=20)  # You can adjust the number of trials depending on your computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial 1:\n",
      "  Value: 0.028469223529100418\n",
      "  Params: {'num_filters': 369, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 70, 'dropout_rate': 0.10084933148470171, 'learning_rate': 4.325122114456011e-05}\n",
      "Best trial 2:\n",
      "  Value: 0.028658822799722355\n",
      "  Params: {'num_filters': 360, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 69, 'dropout_rate': 0.10264323081169152, 'learning_rate': 4.024276019450269e-05}\n",
      "Best trial 3:\n",
      "  Value: 0.028858947878082592\n",
      "  Params: {'num_filters': 360, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 72, 'dropout_rate': 0.11153810108298234, 'learning_rate': 5.362206874806758e-05}\n",
      "Best trial 4:\n",
      "  Value: 0.031221793964505196\n",
      "  Params: {'num_filters': 375, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 64, 'dropout_rate': 0.10224158128898073, 'learning_rate': 0.0001456608120918707}\n",
      "Best trial 5:\n",
      "  Value: 0.03176876219610373\n",
      "  Params: {'num_filters': 397, 'kernel_size': 5, 'pool_size': 4, 'hidden_units': 80, 'dropout_rate': 0.24156398932131568, 'learning_rate': 8.611938425976827e-05}\n"
     ]
    }
   ],
   "source": [
    "# Get the best 5 trials\n",
    "completed_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.COMPLETE])\n",
    "best_trials = sorted(completed_trials, key=lambda t: t.value)[:5]\n",
    "\n",
    "# Print the best 5 trials' parameters and their respective values\n",
    "for i, trial in enumerate(best_trials):\n",
    "    print(f\"Best trial {i + 1}:\")\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(f\"  Params: {trial.params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
